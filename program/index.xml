<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programs on 叶左</title>
    <link>http://yezuozuo.github.io/program/</link>
    <description>Recent content in Programs on 叶左</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 16 Jul 2020 20:02:47 +0800</lastBuildDate>
    
	<atom:link href="http://yezuozuo.github.io/program/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PHP8JIT是什么</title>
      <link>http://yezuozuo.github.io/program/php8jit%E6%98%AF%E4%BB%80%E4%B9%88/</link>
      <pubDate>Thu, 16 Jul 2020 20:02:47 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php8jit%E6%98%AF%E4%BB%80%E4%B9%88/</guid>
      <description>在谈JIT之前我们先回到比较根本的一个问题。
怎么优化 PHP 性能  方案1，迁移到性能更好的语言上，如Go、Java、C++。 方案2，通过 RPC 将功能分离出来用其它语言实现，让 PHP 做更少的事情。 方案3，写 PHP 扩展，在性能瓶颈地方换 C/C++。 方案4，优化 PHP语言本身的性能。  方案1 方案1的问题在于成本问题，如果代码库已经很大的话，迁移是个非常耗费成本的工作，团队将不但放弃多年的经验积累，而且整个团队成员重新学习也是个成本很高的事情。而且PHP语言的数据结构和内置函数，可以几乎直接地描述和处理实际业务，PHP是计算机与现实业务的最直接胶合剂。这也是我一直喜欢PHP的原因。
方案2 方案2是最保险的方案，我们现在也是这样做的，把一些计算比较多的放在Java执行，通过RPC调用。但是PHP本身的机制以及我们实际业务的场景带来的最大问题是IO场景非常多，而PHP本身也没有多线程和异步IO的机制，所以我们现在是依赖RPC提供的并行和异步处理，但这样也不是最优的方案，还是需要等待一定的时长，而这个等待目前来看又是必须的，所以也就是JIT在实际项目中的效果不会特别明显的原因。
方案3 方案3看起来美好，实际执行起来却很难，一般来说性能瓶颈并不会很显著，大多是不断累加的结果，加上 PHP 扩展开发成本高，这种方案一般只用在公共且变化不大的基础库上，所以这种方案解决不了多少问题。
方案4 所以我们只能想方案4，也就是优化PHP的性能了。
更快的 PHP 同样，PHP的性能优化也有很方案：
 方案1，PHP语言层面的优化。 方案2，优化 PHP 的官方实现（也就是 Zend以及JIT）。 方案3，将 PHP 编译成其它语言的 bytecode（字节码），借助其它语言的虚拟机（如 JVM）来运行。 方案4，将 PHP 转成 C/C++，然后编译成本地代码。 方案5，开发更快的 PHP 虚拟机。  方案1 PHP 语言层面的优化是最简单可行的，比如xhprof，也是我们之前包括现在一直在做的事情。但这个也只能优化一部分，并不能很好的解决我们的问题。
方案2 涉及我们的主题，放在后面说。
方案3 开发一个高性能的虚拟机不是件简单的事情，JVM 花了十多年才达到现在的性能，那是否能直接利用这些高性能的虚拟机来优化 PHP 的性能呢？这就是方案3的思路。
虽然看起来很美好，但实际上VM 总是为某个语言优化的，和器官移植一样，其它语言在上面实现会遇到很多瓶颈，比如动态的方法调用，所以这个方案很多大佬都实现并测试过，没有什么好结果。
方案4 它正是 HPHPc（HHVM 的前身）的做法，原理是将 PHP 代码转成 C++，然后编译为本地文件，可以认为是一种 AOT（ahead of time）的方式。</description>
    </item>
    
    <item>
      <title>晚到的PHP8测评</title>
      <link>http://yezuozuo.github.io/program/%E6%99%9A%E5%88%B0%E7%9A%84php8%E6%B5%8B%E8%AF%84/</link>
      <pubDate>Tue, 14 Jul 2020 19:52:48 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E6%99%9A%E5%88%B0%E7%9A%84php8%E6%B5%8B%E8%AF%84/</guid>
      <description>6月25号php 8 alpha 1发布，7月9号 php 8 alpha 2发布，一般alpha 2是一个稍微能用的版本，所以周末测评了一下php8 JIT到底是个什么？
测试方式部分参考鸟哥的blog PHP 8新特性之JIT简介 - 风雪之隅
先看一下结果：
JIT不开启的情况下
php -d opcache.jit_buffer_size=0 Zend/bench.php simple 0.025 simplecall 0.012 simpleucall 0.012 simpleudcall 0.012 mandel 0.135 mandel2 0.211 ackermann(7) 0.082 ary(50000) 0.012 ary2(50000) 0.010 ary3(2000) 0.186 fibo(30) 0.283 hash1(50000) 0.039 hash2(500) 0.041 heapsort(20000) 0.089 matrix(20) 0.110 nestedloop(12) 0.096 sieve(30) 0.045 strcat(200000) 0.019 ------------------------ Total 1.419 JIT开启的情况下：
php -d -d opcache.jit_buffer_size=64M -d opcache.jit=1205 Zend/bench.php simple 0.001 simplecall 0.</description>
    </item>
    
    <item>
      <title>GRPC源码分析</title>
      <link>http://yezuozuo.github.io/program/grpc%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Tue, 14 Jul 2020 14:49:40 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/grpc%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>写在前面 grpc 介绍 grpc 是 google 开源的一款高性能的 rpc 框架。github 上介绍如下：
 gRPC is a modern, open source, high-performance remote procedure call (RPC) framework that can run anywhere
 市面上的 rpc 框架数不胜数，包括 alibaba dubbo 和微博的 motan 等。grpc 能够在众多的框架中脱颖而出是跟其高性能是密切相关的。
CONCEPTS 阅读 grpc 源码之前，我们不妨先了解一些 concepts，github 上也有一些 concepts 介绍
https://github.com/grpc/grpc/blob/master/CONCEPTS.md
1.接口设计  Developers using gRPC start with a language agnostic description of an RPC service (a collection of methods). From this description, gRPC will generate client and server side interfaces in any of the supported languages.</description>
    </item>
    
    <item>
      <title>我为什么喜欢php</title>
      <link>http://yezuozuo.github.io/program/%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88%E5%96%9C%E6%AC%A2php/</link>
      <pubDate>Wed, 01 Jul 2020 13:52:30 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88%E5%96%9C%E6%AC%A2php/</guid>
      <description>会的语言也不少了，但是还是最喜欢PHP。
PHP的最大优势便是他的数据结构和内置函数，具体地说便是字符串和数组，以及字符串和数组的函数。
PHP的字符串既能表示一般文本，也能表示任意二进制数据，也就是说，PHP的字符串就是一段内存。PHP的的字符串操作函数囊括了大部分常见和不常见的文本操作：截取，查找，正则，字符集编码转换……每一个都是一把利器。
PHP的数组是整合了列表和哈希表的数据结构。由于“树”是最能描述现实世界的数据结构，而PHP的数组可以(轻松地)表示任意树，所以，PHP的数组也最能描述现实世界(建模)。
如果要用一句话来描述PHP的优势，我会用这一句：“PHP语言的数据结构和内置函数，可以几乎直接地描述和处理实际业务，PHP是计算机与现实业务的最直接胶合剂。”
说到底我们用计算机语言的目的是为了将现实世界翻译成计算机能懂的语言，将两个世界之间建立联系的方式越简单，他的生命力就越长久。</description>
    </item>
    
    <item>
      <title>计算机中的时间是什么</title>
      <link>http://yezuozuo.github.io/program/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4%E6%98%AF%E4%BB%80%E4%B9%88/</link>
      <pubDate>Sat, 20 Jun 2020 15:03:16 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4%E6%98%AF%E4%BB%80%E4%B9%88/</guid>
      <description>ぎょうき前几天问我，计算机时间咋实现的，为什么写程序的时候能获取到准确的时间，而且全世界的时间都一致。
把我问住了，最先想到的是联网获取的，但是没网怎么办？然后想到是不是系统计算的，那么关机怎么办？难道计算机里有电池？可是电池没电怎么办？
总不可能是计算机感受到地球的自传速度自己算的吧……所以简单整理一下，先看下 Linux 时间处理的一般过程：
应用层 （以下都基于X86体系）
从应用层面，这里我直接用C语言来获取时间。
#include &amp;lt;time.h&amp;gt;int main () { time_t time_raw_format; time(&amp;amp;time_raw_format); // get current time  printf(&amp;#34;time is [%d]\n&amp;#34;, time_raw_format); //use ctime format time  printf(&amp;#34;The current local time: %s&amp;#34;, ctime(&amp;amp;time_raw_format)); return 0; } time is [1592624759] The current local time: Sat Jun 20 11:45:59 2020 很好，时间和我手机上的一模一样。
这里出现的1592624759叫做时间戳，是从 1970年1月1日0点到当前的秒数。一会儿解释下为什么从这个时间开始。
目前我们得到的时间是一个数字，无论精度如何，它代表的仅是一个差值。比如精度为秒的 time() 函数，返回一个 time_t 类型的整数。假设当前时间为2020年6月20日上午11点45分59秒，那么 time_t 的值为：1592624759。即距离1970年1月1日0点，我们已经过去了1592624759秒。（这里的1970年1月1日零点是格林威治时间，而不是北京时间。）我们下面讨论的时间如果不特别说明都是格林威治时间，也叫GMT时间，或者UTC时间。
那么这个时间从哪来的呢？计算机如何实现这一切的呢？在那些应用层 API 和底层系统硬件之间，操作系统和库函数究竟做了些什么？
GlibC层  Glibc是 GNU 发布的libc库，即c 运行库 。glibc是 linux系统 中最底层的 api ，几乎其它任何运行库都会依赖于glibc。glibc除了封装 linux 操作系统所提供的 系统服务 外，它本身也提供了许多其它一些必要功能服务的实现。</description>
    </item>
    
    <item>
      <title>机械同感</title>
      <link>http://yezuozuo.github.io/program/%E6%9C%BA%E6%A2%B0%E5%90%8C%E6%84%9F/</link>
      <pubDate>Tue, 16 Jun 2020 10:53:35 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E6%9C%BA%E6%A2%B0%E5%90%8C%E6%84%9F/</guid>
      <description>机械同感 本篇一部分翻译自Go and CPU Caches - Teiva Harsanyi - Medium，和 【译】CPU 高速缓存原理和应用 - 云+社区 - 腾讯云。
其中代码部分亲自做了基准测试，和作者的有出入，本文会加入我的解释。
 很早很早以前，面试过一个同学，他说他对底层非常感兴趣，看过很多底层设计的书，我问他说这些对日常的工作有什么帮助吗，他说每天的工作就是CURD，底层知识几乎用不到，所以很苦恼。
这里想解释一个词，叫机械同感 (Mechanical Sympathy)。
曾三次获得 F1 世界冠军的杰基* 斯图尔特 (Jackie Stewart) 表示，了解汽车的工作原理让他成为了一名更好的驾驶员。
 “你并不需要先成为一个工程师才能去做一个赛车手，但是你得有一种机械同感”。
 简而言之，了解计算机底层硬件能让我们作为一个更优秀的开发者去设计算法、数据结构等等。
CPU 高速缓存基本原理 现代计算机处理器是基于一种叫对称多处理 (symmetric multiprocessing, SMP) 的概念。在一个 SMP 系统里，处理器的设计使两个或多个核心连接到一片共享内存 (也叫做主存，RAM)。另外，为了加速内存访问，处理器有着不同级别的缓存，分别是 L1、L2 和 L3。确切的体系结构可能因供应商、处理器模型等等而异。然而，目前最流行的模型是把 L1 和 L2 缓存内嵌在 CPU 核心本地，而把 L3 缓存设计成跨核心共享：
越靠近 CPU 核心的缓存，容量就越小，同时访问延迟就越低 (越快)：
同样的，这些具体的数字因不同的处理器模型而异。不过，我们可以做一个粗略的估算：假设 CPU 访问主存需要耗费 60 ns，那么访问 L1 缓存会快上 50 倍。
在处理器的世界里，有一个很重要的概念叫访问局部性 (locality of reference)，当处理器访问某个特定的内存地址时，有很大的概率会发生下面的情况：</description>
    </item>
    
    <item>
      <title>从零开始搭建创业公司后台技术栈</title>
      <link>http://yezuozuo.github.io/program/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E5%88%9B%E4%B8%9A%E5%85%AC%E5%8F%B8%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF%E6%A0%88/</link>
      <pubDate>Fri, 03 Jan 2020 14:07:19 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E5%88%9B%E4%B8%9A%E5%85%AC%E5%8F%B8%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF%E6%A0%88/</guid>
      <description>说到后台技术栈，脑海中是不是浮现的是这样一幅图？
有点眼晕，以上只是我们会用到的一些语言的合集，而且只是语言层面的一部分，就整个后台技术栈来说，这只是一个开始，从语言开始，还有很多很多的内容。今天要说的后台是大后台的概念，放在服务器上的东西都属于后台的东西，比如使用的框架，语言，数据库，服务，操作系统等等，整个后台技术栈我的理解包括4个层面的内容：
 语言： 用了哪些开发语言，如：c++/java/go/php/python/ruby等等； 组件：用了哪些组件，如：MQ组件，数据库组件等等； 流程：怎样的流程和规范，如：开发流程，项目流程，发布流程，监控告警流程，代码规范等等； 系统：系统化建设，上面的流程需要有系统来保证，如：规范发布流程的发布系统，代码管理系统等等；  结合以上的的4个层面的内容，整个后台技术栈的结构如下图所示：
以上的这些内容都需要我们从零开始搭建，在创业公司，没有大公司那些完善的基础设施，需要我们从开源界，从云服务商甚至有些需要自己去组合，去拼装，去开发一个适合自己的组件或系统以达成我们的目标。咱们一个个系统和组件的做选型，最终形成我们的后台技术栈。
一、各系统组件选型 1、项目管理/Bug管理/问题管理 项目管理软件是整个业务的需求，问题，流程等等的集中地，大家的跨部门沟通协同大多依赖于项目管理工具。有一些 SAAS 的项目管理服务可以使用，但是很多时间不满足需求，此时我们可以选择一些开源的项目，这些项目本身有一定的定制能力，有丰富的插件可以使用，一般的创业公司需求基本上都能得到满足，常用的项目如下：
 Redmine： 用 Ruby 开发的，有较多的插件可以使用，能自定义字段，集成了项目管理，BUG 问题跟踪，WIKI 等功能，不过好多插件 N 年没有更新了; Phabricator: 用 PHP 开发的，facebook 之前的内部工具，开发这工具的哥们离职后自己搞了一个公司专门做这个软件，集成了代码托管， Code Review，任务管理，文档管理，问题跟踪等功能，强烈推荐较敏捷的团队使用； Jira：用 Java 开发的，有用户故事，task 拆分，燃尽图等等，可以做项目管理，也可以应用于跨部门沟通场景，较强大； 悟空CRM ：这个不是项目管理，这个是客户管理，之所以在这里提出来，是因为在 To B 的创业公司里面，往往是以客户为核心来做事情的，可以将项目管理和问题跟进的在悟空 CRM 上面来做，他的开源版本已经基本实现了 CR&amp;lt; 的核心 功能，还带有一个任务管理功能，用于问题跟进，不过用这个的话，还是需要另一个项目管理的软件协助，顺便说一嘴，这个系统的代码写得很难维护，只能适用于客户规模小（1万以内）时。  2、DNS DNS 是一个很通用的服务，创业公司基本上选择一个合适的云厂商就行了，国内主要是两家：
 阿里万网：阿里 2014 年收购了万网，整合了其域名服务，最终形成了现在的阿里万网，其中就包含 DNS 这块的服务； 腾讯 DNSPod: 腾讯 2012 年以 4000 万收购 DNSPod 100% 股份，主要提供域名解析和一些防护功能；  如果你的业务是在国内，主要就是这两家，选 一个就好，像今日头条这样的企业用的也是 DNSPod 的服务，除非一些特殊的原因才需要自建，比如一些 CDN 厂商，或者对区域有特殊限制的。要实惠一点用阿里最便宜的基础版就好了，要成功率高一些，还是用DNSPod 的贵的那种。</description>
    </item>
    
    <item>
      <title>php性能优化之并行与异步</title>
      <link>http://yezuozuo.github.io/program/php%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B9%8B%E5%B9%B6%E8%A1%8C%E4%B8%8E%E5%BC%82%E6%AD%A5/</link>
      <pubDate>Sun, 05 May 2019 21:35:29 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B9%8B%E5%B9%B6%E8%A1%8C%E4%B8%8E%E5%BC%82%E6%AD%A5/</guid>
      <description>PHP性能优化之并行与异步 在我们的一个核心接口中会请求大量的RPC服务，用来获取各种数据，比如一个接口一次请求将会产生平均7~8次RPC，调用虽然每个接口都非常的快（ms级），但8次累加起来的消耗还是相当的可观，所以我最近的优化工作主要是:
 通过某种方式并行（异步）调用各RPC请求，以缩短执行时间。
 当我开始接手这项工作的时候，脑海中想到的第一个对应思想就是Lazy evaluation（ 缓式求值 ），维基百科上对于缓式求值的定义是：
 In programming language theory, lazy evaluation, or call-by-need[1] is an evaluation strategy which delays the evaluation of an expression until its value is needed (non-strict evaluation) and which also avoids repeated evaluations (sharing).
 Lazy evaluation是编程语言设计领域中的一个表达式求值策略，它延缓对表达式的求值直到你需要它的时候。看上去lazy evaluation好像和我们的问题挨不上边，而且php也不支持 lazy evaluation，不过仔细想一下，如果我们能把对RPC请求的后续操作延缓到对返回结果的使用时，就可以用一种优雅的实现来使框架支持并行执行，而且对于业务层的改动也非常的小。
具体点说就是在进行RPC调用的时候，不再返回结果，而是返回一个句柄，这个句柄标识了一个被提交到后台的请求，它被加入到一个变量中，你不再关心它，由其他服务替你完成，你的代码可以继续往下执行，去完成其他的业务逻辑。而当我们需要这个结果时，检查这个句柄是否已经完成，如果已经完成则执行接受结果之后的所有操作，返回结果。
解决这样的问题大概有这样几种方式：多线程、协程、如果是HTTP请求的话可以用CURL提供的 multi*方法。
但是，在我们这里统统不适用，PHP语言本身对并行的处理能力有限，需要借助其他的服务。
方案 我们会在调用的时候，将多个RPC请求进行合并，统一发送给一个新的RPC服务（暂时称作Multi RPC）,Multi RPC将以代理的身份，并行获取多个数据。处理完成或者整体超时后，会将所有数据返回，由API进行下一步的处理。
关键问题  超时问题，使用最大超时，调用方比指定超时多设置10ms 服务拆分逻辑，对互不依赖代码进行拆分 提前打包后缓存再取，还是当次请求直接返回  协议方案 service uri, method, protocal, params, timeout, retry</description>
    </item>
    
    <item>
      <title>名词解释</title>
      <link>http://yezuozuo.github.io/program/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/</link>
      <pubDate>Sat, 20 Apr 2019 14:55:48 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/</guid>
      <description>布隆过滤器 (Bloom Filter)是由Burton Howard Bloom于1970年提出，它是一种space efficient的概率型数据结构，用于判断一个元素是否在集合中。在垃圾邮件过滤的黑白名单方法、爬虫(Crawler)的网址判重模块中等等经常被用到。哈希表也能用于判断元素是否在集合中，但是布隆过滤器只需要哈希表的1/8或1/4的空间复杂度就能完成同样的问题。布隆过滤器可以插入元素，但不可以删除已有元素。其中的元素越多，false positive rate(误报率)越大，但是false negative (漏报)是不可能的。
C10k:指的是服务器同时支持成千上万个客户端的问题，也就是concurrent 10000 connnection。
彩虹表：彩虹表是一个用于加密散列函数逆运算的预先计算好的表, 常用于破解加密过的密码散列。一般主流的彩虹表都在100G以上。 查找表常常用于包含有限字符固定长度纯文本密码的加密。这是以空间换时间的典型实践, 在每一次尝试都计算的暴力破解中使用更少的计算能力和更多的储存空间，但却比简单的每个输入一条散列的翻查表使用更少的储存空间和更多的计算性能。使用加盐的KDF函数可以使这种攻击难以实现。
2Z原则
Zoomin聚焦，指对事实和细节进行详情、透彻的分析。
Zoom out退一步，从全局来看，需要跳出数据和细节来分析全貌。
php的zts太弱，不要用。
开发swoole和开发php是两种不同的概念，默念一百次生命周期不一样。
如果sleep 100秒，万一10秒时窗口关闭了 100秒后还会执行代码吗？http://php.net/manual/en/misc.configuration.php#ini.ignore-user-abort 浏览器关闭，php是否继续执行，受到此参数的影响。默认是浏览器关闭，php不会继续执行，当然不是立即停止，而是完成一次系统调用之后才会停止。至于怎么认为一次系统调用，就是触发一次 declare(ticks=1); sleep 100; 关闭浏览器php不会立即停止，可能到php完全结束了，是没有触发系统调用。死循环 sleep 1; 循环了几次，关闭浏览器之后，延迟几十或几百毫秒php停止。
简而言之，其他常规语言使用字母 “ymd” 编写时间格式，而 golang 使用固定的时刻 &amp;ldquo;2006-01-02 15:04:05&amp;rdquo; 编写时间格式。 优点就是所见即所得，立刻看到效果，缺点就是，你要记住这个时刻，这是目前为止遇到的第一个挑战。至于为什么是这个时时刻？它有什么意义？我不知道。
mysql rank居然是个函数，所以字段名不能叫rank，sql的报错居然体现不出来，fuck
P对象(processor) 代表上下文（或者可以认为是cpu），M(work thread)代表工作线程，G对象（goroutine）
Time工具用于初步分析程序性能很有用，real项就是程序执行的时间，user 项就是好在用户态CPU的时间，sys 表示内核态CPU的时间。real - user - sys不一定是0，这正好是程序IOWait的时间。</description>
    </item>
    
    <item>
      <title>php使用redis连接数过高问题</title>
      <link>http://yezuozuo.github.io/program/php%E4%BD%BF%E7%94%A8redis%E8%BF%9E%E6%8E%A5%E6%95%B0%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/</link>
      <pubDate>Tue, 19 Mar 2019 13:07:02 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E4%BD%BF%E7%94%A8redis%E8%BF%9E%E6%8E%A5%E6%95%B0%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98/</guid>
      <description>最近发生了几次php连接数过高导致的问题，由于业务体量上涨导致php机器扩容，某个库的连接数过高导致redis cpu跑满。
系统解释一下php为什么会出现连接过高的问题。
php进程模型 根据上图能够得到：
 每个redis实例的连接数 = php的实例数 * 每个实例上的进程数
 比如有200个redis实例，每个php实例上有100个进程，那么每个redis上会存在 200 * 100 = 2w个连接。
当请求量上涨时，php业务需要扩容，扩容之后会加大redis的连接数
(200 + 100)个php实例 * 100 进程数 = 3w连接
经过一次故障，对于redis库我们大概得出的结论是：
 单个redis实例所能承担的连接数上限为5w，3w以下认为无风险，3w-4w之间存在风险，4w-5w之间出现超时，对业务产生影响，超过5w redis server的cpu会上升直至100%，redis开始拒绝服务。
 （其他服务有待测试，某单实例连接数6w业务层没有异常，虽然redis号称10w连接无问题，实际因为命令不一样，所用cpu不一样）
所以现有的php服务面临了一个困境，业务扩容会导致redis的连接数上涨，redis的连接数上涨又会拖慢业务，进入了一个死锁。
解释了每次php负载上升都同时会有redis连接数的各种报警
疑问 Q：扩容redis有没有效果，我扩10000个redis难道也解决不了吗
A：很遗憾，是没有效果的，此举反而会加重php实例的连接数，如下图
 redis的连接数只和php的实例数以及每个实例上的进程数相关，所以单实例的连接数不会变小，同时由于redis的实例数增多，会使php建立的连接数增多。
 解决方案  php层建立连接池，现在的连接是进程级别复用的，而非实例级别复用  但是php没有非常成熟的本地连接池，有一些例如swoole等，但是测试起来需要成本   php连接集群时单个进程只和一个实例建立连接，例如php的32333进程只和39400端口建立连接  这种情况只适用于队列的方式，其他情况下不适合   业务拆分，将s1改为s1+s2从库，业务上根据不同业务连接不同从库，在解决连接数过高的时候临时采用了这个方案，是有效果的，如下图  问题：但非长久之计，业务体量上涨迟早有一天s1和s2的连接数也会到达瓶颈
 php和redis之间建立proxy层，类似codis，twemproxy  会有性能损耗，但是从长期来看是一个根本解决问题的方法    一个故障是如何发生的 </description>
    </item>
    
    <item>
      <title>Java一些记录</title>
      <link>http://yezuozuo.github.io/program/java%E4%B8%80%E4%BA%9B%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Fri, 22 Feb 2019 14:12:27 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/java%E4%B8%80%E4%BA%9B%E8%AE%B0%E5%BD%95/</guid>
      <description>线程安全的优先级高于性能。
Java中的多线程你只要看这一篇就够了 - ImportNew
Java 之 synchronized 详解 - 掘金
Java并发编程：volatile关键字解析 - Matrix海子 - 博客园
缓存使用过程中最重要问题是什么时候创建缓存和缓存的失效机制。缓存可以在第一次获取的时候创建也可以在程序启动和缓存失效之后立即创建，缓存的失效可以定期失效，也可以在数据发生变化的时候失效，如果按数据发生变化让缓存失效，还可以分粗粒度失效和细粒度失效。
网站架构的整个演变过程主要是围绕大数据和高并发这两个问题展开的，解决的方案主要分为使用缓存和使用多资源两种类型。多资源主要指多存储（包括多内存）、多CPU和多网络，对于多资源来说又可以分为单个资源处理一个完整的请求和多个资源合作处理一个请求两种类型，如多存储和多CPU中的集群和分布式，多网络中的CDN和静态资源分离。理解了整个思路之后就抓住了架构演变的本质，而且自己可能还可以设计出更好的架构。
无论架构还是协议都要以正确的态度对待，它们都是为了解决特定的问题而设计出来的，我们要认真并且谦虚地学习，不过也不需要将它们当成神圣不可侵犯的东西，它们的本质还是为我们解决问题的工具。另外这些架构、协议以及相关的产品都是经过实际的考验可以解决问题的，不过也并不是说它们就是最优的解决方案，我们只有真正理解了它们所针对的问题才能对它们理解得更透彻、使用得更灵活。
记录类型：域名解析有很多种解析的类型，如常用的A记录和CNAME记录。A记录是将域名解析到IP（一个域名可以有多条A记录），CNAME记录是将域名解析到另一个域名（也就是作为另一个域名的别名），查找时会返回目标域名所对应的IP，
① www.excelib.com 可能会有多条解析记录，而 excelib.com 用一条CNAME记录就可以完成了；②在 www.excelib.com 的IP发生变化时， excelib.com 不需要修改解析内容直接就可以自动改变；③使用CDN时可以将用户直接访问的域名作为一个别名，然后将指向的域名通过ns记录指定CDN专用的DNS服务器进行解析，这样用户访问的域名解析使用的还是正常的DNS服务器但是可以获取到CDN的DNS服务器解析的结果。
Servlet是Server+Applet的缩写，表示一个服务器应用。通过上面的分析我们知道Servlet其实就是一套规范，我们按照这套规范写的代码就可以直接在Java的服务器上面运行。Servlet3.1中Servlet的结构如图6-1所示。
Arrays类为所有基本数据类型的数组提供了一个过载的sort()和binarySearch()，它们亦可用于String和Object。
类加载有三种方式：  1、命令行启动应用时候由JVM初始化加载 2、通过Class.forName()方法动态加载 3、通过ClassLoader.loadClass()方法动态加载  Class.forName()和ClassLoader.loadClass()区别  Class.forName()：将类的.class文件加载到jvm中之外，还会对类进行解释，执行类中的static块； ClassLoader.loadClass()：只干一件事情，就是将.class文件加载到jvm中，不会执行static中的内容,只有在newInstance才会去执行static块。 Class.forName(name,initialize,loader)带参函数也可控制是否加载static块。并且只有调用了newInstance()方法采用调用构造函数，创建类的对象 。  减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。
·无锁并发编程。多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一 些办法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据。 ·CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁。 ·使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这 样会造成大量线程都处于等待状态。 ·协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。
是值传递。Java语言的方法调用只支持参数的值传递。
String和StringBuilder、StringBuffer的区别？
答：Java平台提供了两种类型的字符串：String和StringBuffer/StringBuilder，它们可以储存和操作字符串。其中String是只读字符串，也就意味着String引用的字符串内容是不能被改变的。而StringBuffer/StringBuilder类表示的字符串对象可以直接进行修改。StringBuilder是Java 5中引入的，它和StringBuffer的方法完全相同，区别在于它是在单线程环境下使用的，因为它的所有方面都没有被synchronized修饰，因此它的效率也比StringBuffer要高。
Java中的线程可以分为守护线程（Daemon Thread）和用户线程（User Thread）。用户线程会阻止JVM的正常停止，即JVM正常停止前应用程序中的所有用户线程必须先停止完毕；否则JVM无法停止。而守护线程则不会影响JVM的正常停止，即应用程序中有守护线程在运行也不影响JVM的正常停止。因此，守护线程通常用于执行一些重要性不是很高的任务，例如用于监视其他线程的运行情况。
Java语言中，子线程是否是一个守护线程取决于其父线程：默认情况下父线程是守护线程则子线程也是守护线程，父线程是用户线程则子线程也是用户线程。当然，父线程在创建子线程后，启动子线程之前可以调用Thread实例的setDaemon方法来修改线程的这一属性。
从上述描述可知，一个线程在其整个生命周期中，只可能一次处于NEW状态和TERMINATED状态。而一个线程的状态从RUNNABLE状态转换为BLOCKED、WAITING和TIMED_WAITING这几个状态中的任何一个状态都意味着上下文切换（Context Switch）的产生。
synchronized关键字可以实现操作的原子性，其本质是通过该关键字所包括的临界区（Critical Section）的排他性保证在任何一个时刻只有一个线程能够执行临界区中的代码，这使得临界区中的代码代表了一个原子操作。这一点，读者可能已经很清楚。但是，synchronized关键字所起的另外一个作用——保证内存的可见性（Memory Visibility），也是值得我们回顾的。</description>
    </item>
    
    <item>
      <title>cmd=NULL空连接的排查方式</title>
      <link>http://yezuozuo.github.io/program/cmdnull%E7%A9%BA%E8%BF%9E%E6%8E%A5%E7%9A%84%E6%8E%92%E6%9F%A5%E6%96%B9%E5%BC%8F/</link>
      <pubDate>Mon, 18 Feb 2019 20:13:02 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/cmdnull%E7%A9%BA%E8%BF%9E%E6%8E%A5%E7%9A%84%E6%8E%92%E6%9F%A5%E6%96%B9%E5%BC%8F/</guid>
      <description>现象 dba收到连接数过多的报警，在redis上client list看有哪些连接，如下图，发现大多数都是cmd=NULL的连接，cmd=NULL一般是客户端建立了连接，但是没有任何操作。
初步排查 要么是redis问题，要么是业务问题。
登上php机器，找一个端口查看连接的建立情况，如下图：
然后持续的看同一个端口的建联情况，发现过一会儿就没有这个连接了。
目前为止得到的结论：
 连接是会断开的，现在的问题是一直有新连接在建立，但是建立完没有任何行为。 这么来看大概率是业务代码的问题。
 对业务进行排查 发现php代码里面有
public function __construct($ctx){ $this-&amp;gt;ctx = $ctx; $this-&amp;gt;expireRedis = $this-&amp;gt;ctx-&amp;gt;expireRedis; }  在构造方法里创建了一个redis对象的实例，有些php的开发人员为了context的写法简单会使用这种方式。 但是该class下有非常多的方法，如果有的请求不用这个redis，而只是用到了class中的其他方法，就会导致白白创建了一个连接。
 看一下创建的时候都做了什么
public function getExpireRedis() { return $this-&amp;gt;ctx-&amp;gt;redis-&amp;gt;getClient(&#39;host&#39;, &#39;port&#39;); } public function getClient($host, $port, $time_out = 3) { return new Redis_Client($host, $port, $time_out); } Redis_Client在构造方法里就会进行对host和port的connect，所以一旦创建了redis的实例，那么就会进行连接，如果之后对这个redis实例什么都不做的话，那么就会在一段时间内，redis server就会存在一条cmd=NULL的连接。
复现 为了证实上述的说法，我们进行一下简单的复现
php-fpm的方式下，对下述代码进行请求：
$redis = new Redis(); for ($i = 1; $i &amp;lt; 10000;$i++) { $redis = new Redis(); $redis-&amp;gt;pconnect(&#39;127.</description>
    </item>
    
    <item>
      <title>php中的trait</title>
      <link>http://yezuozuo.github.io/program/php%E4%B8%AD%E7%9A%84trait/</link>
      <pubDate>Mon, 03 Dec 2018 21:31:49 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E4%B8%AD%E7%9A%84trait/</guid>
      <description>为什么要使用trait?
先来看一个情景，如果想要写一个继承模型该怎么做。
最常见的一种时创建一个父类，让子类来继承父类。这个方法有一个问题，就是有可能子类之间是无关的，只是为了继承而写在一起，逻辑上说不通。
第二种是创建一个接口，然后类来实现这个接口，这种方法比第一种方法要好，但是在两个类中有可能会重复实现相同的功能，违背了DRY原则（Don’t Repeat Yourself）。
最后也是最好的一种方法就是创建trait，定义并实现公有的方法，然后需要的时候混入这个trait即可。这么做不会搅乱子类原有的自然继承层次结构。
PHP解释器在编译时会把trait复制粘贴到类的定义体中，但是不会处理这个操作引入的不兼容问题。如果trait假定类中有特定的属性活着方法（在trait中没有定义），要确保相应的类中有对应的属性和方法。</description>
    </item>
    
    <item>
      <title>php Switch的一个注意点</title>
      <link>http://yezuozuo.github.io/program/php-switch%E7%9A%84%E4%B8%80%E4%B8%AA%E6%B3%A8%E6%84%8F%E7%82%B9/</link>
      <pubDate>Mon, 23 Jul 2018 18:49:11 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php-switch%E7%9A%84%E4%B8%80%E4%B8%AA%E6%B3%A8%E6%84%8F%E7%82%B9/</guid>
      <description>public function formatTime($timestamp) { $now = time(); $diff = $now - $timestamp; switch ($diff) { case $diff &amp;lt; 60: $re = &#39;1分钟&#39;; break; case $diff &amp;lt; 60 * 60: $re = strval(intval($diff / 60)) . &#39;分钟&#39;; break; case $diff &amp;lt; 60 * 60 * 48: $re = strval(intval($diff / (60 * 60))) . &#39;小时&#39;; break; default: $re = strval(intval($diff / (60 * 60 * 24))) . &#39;天&#39;; break; } return $re; } 当时间戳是time()的时候，diff为0，想的得到的结果应该是返回1分钟，结果却返回了0天。</description>
    </item>
    
    <item>
      <title>一个php编译器的问题</title>
      <link>http://yezuozuo.github.io/program/%E4%B8%80%E4%B8%AAphp%E7%BC%96%E8%AF%91%E5%99%A8%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sat, 23 Jun 2018 19:39:29 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E4%B8%80%E4%B8%AAphp%E7%BC%96%E8%AF%91%E5%99%A8%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>看一个问题$a=1; $a+$a++ 的值是多少
答案是3，根据下图，先执行$a++,在执行+。
$a + ($a++) = 3
那么$a+$a+$a++呢？是4吗
看下图
答案还是3，执行过程是($a+$a) + ($a++) = 3。
那么$a+$a+$a+$a++呢？是4吗？
答案也不是5，答案是4
(($a+$a)+$a) + ($a++) = 4。
原因：
也就是 这个问题没啥意义…… 每个编译器都不一样。</description>
    </item>
    
    <item>
      <title> php 7.2.1 &#43; redis 3.1.6 超时问题</title>
      <link>http://yezuozuo.github.io/program/php-7.2.1-&#43;-redis-3.1.6-%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/</link>
      <pubDate>Wed, 23 May 2018 19:50:52 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php-7.2.1-&#43;-redis-3.1.6-%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/</guid>
      <description>在升级php 7.2.1之后发现某集群的超时时间骤增，从平均80ms变成了120ms。
同时在报错日志中发现了较多的如下错误：
本来设置的超时都没生效，可能导致性能下降。
然后strace看一下进程执行记录：
发现poll的时候的timeout都是60000，而php 7.0.13 + redis 3.0.0的机器的timeout都是pconnect对应的时间，由此可以看出pconnect传的参数在这个版本搭配下并未生效。
看一下源码：
pconnect-&amp;gt;redis_sock_create-&amp;gt;redis_sock_server_open-&amp;gt;redis_sock_connect。
 tv.tv_sec = (time_t)redis_sock-&amp;gt;timeout; tv.tv_usec = (int)((redis_sock-&amp;gt;timeout - tv.tv_sec) * 1000000); if(tv.tv_sec != 0 || tv.tv_usec != 0) { tv_ptr = &amp;amp;tv; }  redis_sock-&amp;gt;stream = php_stream_xport_create(host, host_len, 0, STREAM_XPORT_CLIENT | STREAM_XPORT_CONNECT, persistent_id, tv_ptr, NULL, NULL, &amp;amp;err); 在php源码里看一下php_stream_xport_create
PHPAPI php_stream *_php_stream_xport_create(const char *name, size_t namelen, int options, int flags, const char *persistent_id, struct timeval *timeout, php_stream_context *context, zend_string **error_string, int *error_code STREAMS_DC) { struct timeval default_timeout = { 0, 0 }; default_timeout.</description>
    </item>
    
    <item>
      <title>基于Upsync模块实现Nginx动态配置</title>
      <link>http://yezuozuo.github.io/program/%E5%9F%BA%E4%BA%8Eupsync%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0nginx%E5%8A%A8%E6%80%81%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Sun, 22 Apr 2018 10:14:33 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E5%9F%BA%E4%BA%8Eupsync%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0nginx%E5%8A%A8%E6%80%81%E9%85%8D%E7%BD%AE/</guid>
      <description>Upsync是新浪微博开源的基于Nginx实现动态配置的三方模块。Nginx-Upsync-Module的功能是拉取Consul的后端server的列表，并动态更新Nginx的路由信息。此模块不依赖于任何第三方模块。Consul作为Nginx的DB，利用Consul的KV服务，每个Nginx Work进程独立的去拉取各个upstream的配置，并更新各自的路由。
Upsync模块工作原理 在Nginx的设计中，每一个upstream维护了一张静态路由表，存储了backend的ip、port以及其他的meta信息。每次请求到达后，会依据location检索路由表，然后依据具体的调度算法(如round robin )选择一个backend转发请求。但这张路由表是静态的，如果变更后，则必须reload，经常reload的话这对SLA有较大影响。
为了达到减少reload的目的，大多通过动态更新维护路由表来解决这个问题。通常路由表的维护有Push与Pull两种方式。
Push方案 通过Nginx API向Nginx发出请求,操作简单、便利。
架构图如下：
http api除了操作简单、方便，而且实时性好；缺点是有多台Nginx时，不同Nginx路由表的一致性难于保证，如果某一条注册失败，便会造成服务配置的不一致，容错复杂。另外扩容Nginx服务器，需要从其他的Nginx中同步路由表。
Pull方案 路由表中所有的backend信息(含meta)存储到Consul，所有的Nginx从Consul拉取相关信息。有变更则更新路由表，利用Consul解决一致性问题，同时利用Consul的wait机制解决实时性问题。利用Consul的index进行增量摘取，解决带宽占用问题。
在Consul中，一个K/V对代表一个backend信息，增加一个即视作扩容，减少一个即为缩容。调整meta信息，如权重，也可以达到动态流量调整的目的。
架构图如下：
基于动态路由的方案实现 Upsync模块使用了第二种模式，通过拉取Consul的后端Server的列表，并动态更新Nginx的路由信息。Upsync模块工作流程图如下：
每个Work进程定时的去Consul拉取相应upstream的配置，若Consul发现对应upstream的值没有变化，便会hang住这个请求五分钟(默认值)。在这五分钟内对此upstream的任何操作，都会立刻返回给Nginx对相应路由进行更新。
upstream变更后，除了更新Nginx的缓存路由信息，还会把本upstream的后端server列表dump到本地，保持本地server信息与consul的一致性。
除了注册／注销后端的server到consul，会更新到Nginx的upstream路由信息外，对后端server属性的修改也会同步到nginx的upstream路由。
Upsync模块支持修改的属性有：weight、max_fails、fail_timeout、down。
 修改server的权重可以动态的调整后端的流量。 若想要临时移除server，可以把server的down属性置为1。 若要恢复流量，可重新把down置为0。  每个work进程各自拉取、更新各自的路由表，采用这种方式的原因：
 基于Nginx的进程模型，彼此间数据独立、互不干扰。 若采用共享内存，需要提前预分配，灵活性可能受限制，而且还需要读写锁，对性能可能存在潜在的影响。 若采用共享内存，进程间协调去拉取配置，会增加它的复杂性，拉取的稳定性也会受到影响。  Upsync模块高可用性
Nginx的后端列表更新依赖于Consul，但是不强依赖于它，具体表现为：
 即使中途Consul意外挂了，也不会影响Nginx的服务，Nginx会沿用最后一次更新的服务列表继续提供服务。 若Consul重新启动提供服务，这个时候Nginx会继续去Consul探测，这个时候Consul的后端服务列表发生了变化，也会及时的更新到Nginx。 work进程每次更新都会把后端列表dump到本地，目的是降低对Consul的依赖性，即使在consul不可用时，也可以Reload Nginx。   Nginx启动流程图如下：
Nginx启动时，master进程首先会解析本地的配置文件，解析完成功，接着进行一系列的初始化，之后便会开始work进程的初始化。work初始化时会去Consul拉取配置，进行work进程upstream路由信息的更新，若拉取成功，便直接更新，若拉取失败，便会打开配置的dump后端列表的文件，提取之前dump下来的server信息，进行upstream路由的更新，之后便开始正常的提供服务。
每次去拉取Consul都会设置连接超时，由于Consul在无更新的情况下默认会hang五分钟，所以响应超时配置时间应大于五分钟。大于五分钟之后，Consul依旧没有返回，便直接做超时处理。</description>
    </item>
    
    <item>
      <title>php连接检查</title>
      <link>http://yezuozuo.github.io/program/php%E8%BF%9E%E6%8E%A5%E6%A3%80%E6%9F%A5/</link>
      <pubDate>Tue, 06 Mar 2018 19:45:26 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E8%BF%9E%E6%8E%A5%E6%A3%80%E6%9F%A5/</guid>
      <description>长连接可以减少建立连接的过程, 使用长连接可以提高服务的性能。php 很多扩展都支持长连接，如 redis, memcache, mysql 的主流扩展都支持。
我们知道长连接就是一次建立连接，使用之后不会马上释放，而是把这个连接放到连接池。那么引发的一个问题就是，我们下次使用时如何知道这个连接是否已经被关闭。
我们来看看 phpredis 是如何来判断，连接是否可用。 phpredis 检查的函数在 library.c 的 redis_check_eof 的方法，而这个方法调用的是 php 内部的方法 php_stream_eof, 我们来看这个方法的具体实现。
PHPAPI int _php_stream_eof(php_stream *stream TSRMLS_DC) { // 如果有数据未读取，说明 socket 还是可用 if (stream-&amp;gt;writepos - stream-&amp;gt;readpos &amp;gt; 0) { return 0; } // 咦? 这里通过 php_stream_set_option 来检查 if (!stream-&amp;gt;eof &amp;amp;&amp;amp; PHP_STREAM_OPTION_RETURN_ERR == php_stream_set_option(stream, PHP_STREAM_OPTION_CHECK_LIVENESS, 0, NULL)) { stream-&amp;gt;eof = 1; } return stream-&amp;gt;eof; } 判断socket 是否可用, 有两个条件:
 writepos &amp;gt; readpos, 说明还有数据未读, 连接正在使用中 php_stream_set_option 通过PHP_STREAM_OPTION_CHECK_LIVENESS 选项来判断  解析来看看 php_stream_set_option 是如何实现的:</description>
    </item>
    
    <item>
      <title>调redis就进程crash</title>
      <link>http://yezuozuo.github.io/program/%E8%B0%83redis%E5%B0%B1%E8%BF%9B%E7%A8%8Bcrash/</link>
      <pubDate>Fri, 23 Feb 2018 11:23:44 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E8%B0%83redis%E5%B0%B1%E8%BF%9B%E7%A8%8Bcrash/</guid>
      <description>[07-Feb-2018 18:50:06] WARNING: [pool www] child 597 exited on signal 11 (SIGSEGV) after 6502.027154 seconds from start [07-Feb-2018 18:50:06] NOTICE: [pool www] child 640 started 多次测试后必现。
 SIGSEGV是当一个进程执行了一个无效的内存引用，或发生段错误时发送给它的信号。
 SIGSEGV会导致php进程的crash，同时观察php的存货进程，确定每次调用该接口的时候所在进程会被kill掉。
查一下php的代码调用什么会造成这个结果，最终定位到的代码是：
$redis-&amp;gt;zRevRangeByScore($key,&#39;+inf&#39;,&#39;0&#39;,[&#39;withscores&#39;=&amp;gt;false,&#39;limit&#39;=&amp;gt;1111]); 这个的确是官方的写法，但是我在物理机上执行这个命令的时候，却没有crash。
查看系统环境：php版本是一致的，redis的版本不一致，docker上是3.0.0，物理机上是3.1.2，根据经验很可能是redis版本造成的问题。
通过以下方式可以暂时解决这个问题
 zRevRangeByScore换一种写法 换redis版本  从根本上看一下这个问题的原因：
先strace看一下日志：
可以看到从redis已经返回数据了，但是返回数据之后马上收到一个SIGSEGV的报错，还看不出报错的具体原因。
再用gdb看一下：
可以看出报错是发生在zend_hash_index_find()函数上，在执行这个函数的时候发生了内存错误。
看一下最新的phpredis和phpredis3.0.0在这个函数上的对比：
3.0.0:
 // Check for an options array if(z_opt &amp;amp;&amp;amp; Z_TYPE_P(z_opt)==IS_ARRAY) { ht_opt = Z_ARRVAL_P(z_opt); // Check for WITHSCORES *withscores = ((z_ele = zend_hash_str_find(ht_opt,&amp;quot;withscores&amp;quot;,sizeof(&amp;quot;withscores&amp;quot;) - 1)) !</description>
    </item>
    
    <item>
      <title>APP API需要同时维护多个版本的一些想法</title>
      <link>http://yezuozuo.github.io/program/app-api%E9%9C%80%E8%A6%81%E5%90%8C%E6%97%B6%E7%BB%B4%E6%8A%A4%E5%A4%9A%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/</link>
      <pubDate>Sat, 20 Jan 2018 20:13:02 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/app-api%E9%9C%80%E8%A6%81%E5%90%8C%E6%97%B6%E7%BB%B4%E6%8A%A4%E5%A4%9A%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/</guid>
      <description>第一种形式：
Controller/V1.0.0/ -----------------/UserController.php -----------------/UploadController.php Controller/V2.1.0/ -----------------/UserController.php -----------------/UploadController.php 第二种形式：
Controller/ ----------/UserCreateController.php ----------/UserInfoController.php ----------/UploadImageController.php UserCreateController.php 内容如下：
classUserCreateextendsApiController{ publicfunctionv1_0_0(){} publicfunctionv2_0_0(){} } 第三种形式：
客户端在做请求的时候在接口中添加version字段，标识出请求的是哪个接口：
api.xxx.com/api?version=v1&amp;amp;&amp;hellip; api.xxx.com/api?version=v2&amp;amp;&amp;hellip;
这种做起来比较简单也容易理解，但是在你的每个接口逻辑里面都得需要写判断版本的代码了。比如：
if(version == &#39;v1&#39;) { do_something_with_v1_style }else if (version ==&#39;v2&#39;) { do_something_with_v2_style } 这样的代码看起来感觉很不舒服。而且会维护一大堆的if-else，以后会越来越长。
第四种形式：
客户端在做请求的时候在HTTP HEAD里面中添加API-VERSION字段，标识出请求的是哪个接口：
-H&amp;quot;API-VERSION: v1&amp;quot; -H&amp;quot;API-VERSION: v2&amp;quot; 这个需要统一做的事情稍微有点多，但之后的接口逻辑会比较好些。在入口的地方获取接口版本，然后把请求分发到对应版本的接口处理器上。
api(req): if(req.HEADS[&amp;quot;API-VERSION&amp;quot;] ==&#39;v1&#39;) { distribute_to_v1_api(req) } else if (ver ==&#39;v2&#39;) { distribute_to_v2_api(req) } 第五种形式：
不同版本使用不同的域名，这样：
v1.api.xxx.com
v2.api.xxx.com
域名的方式可以采用下面的两种方式：
1、不同版本的api部署成不同的应用（甚至可以部署到不同的服务器上），彼此间独立，其好处是部署的过程不会影响其他版本api的使用，并且可以减轻单台服务器的负担。 2、部署在一个应用上面，但是和第四种一样，在接口入口出分发到不同版本的接口处理器上进行处理。好处是不同版本间能够直接复用相同的功能。
总结一下：
我个人比较倾向于第一种(xxx.com/v1/、xxx.com/v2/)：
在整个产品的生命周期中接口的数目和功能可能会不停的增加，但对于某个接口而言，不会频繁的变动（修改接口的输入输出约定），而增加接口对于老的接口是没有影响的，也就不会到必须升级接口的地步（你的老app只是在用原来就存在的老接口而已，新增加的接口对它没有影响）。
如果你的接口变化已经到了今天v1、明天v2、后天v3的地步，那么得考虑你们一开始对产品的需求是否足够准确了（估计需要维护的接口文档也会让人头疼）。
不同版本接口相互独立在某种程度上限制了你，让你不会随随便便就v1、v2、v3。（当你每天都要用一个新域名的时候你自己一定会不自然的反思是不是变换太频繁了）。
接口版本信息能够直接在url里面体现，清晰易懂，也比较容易做接口调试（没错，给我一个Chrome就够了）。</description>
    </item>
    
    <item>
      <title>Linux Slab分配器</title>
      <link>http://yezuozuo.github.io/program/linux-slab%E5%88%86%E9%85%8D%E5%99%A8/</link>
      <pubDate>Fri, 29 Dec 2017 10:17:49 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/linux-slab%E5%88%86%E9%85%8D%E5%99%A8/</guid>
      <description>动态内存管理 内存管理的目标是提供一种方法，为实现各种目的而在各个用户之间实现内存共享。内存管理方法应该实现以下两个功能：
 最小化管理内存所需的时间 最大化用于一般应用的可用内存（最小化管理开销）  内存管理实际上是一种关于权衡的零和游戏。可以开发一种使用少量内存进行管理的算法，但是要花费更多时间来管理可用内存。也可以开发一个算法来有效地管理内存，但却要使用更多的内存。最终，特定应用程序的需求将促使对这种权衡作出选择。
每个内存管理器都使用了一种基于堆的分配策略。在这种方法中，大块内存（称为堆）用来为用户定义的目的提供内存。当用户需要一块内存时，就请求给自己分配一定大小的内存。堆管理器会查看可用内存的情况（使用特定算法）并返回一块内存。搜索过程中使用的一些算法有first-fit（在堆中搜索到的第一个满足请求的内存块）和best-fit（使用堆中满足请求的最合适的内存块）。当用户使用完内存后，就将内存返回给堆。
这种基于堆的分配策略的根本问题是碎片（fragmentation）。当内存块被分配后，它们会以不同的顺序在不同的时间返回。这样会在堆中留下一些洞，需要花一些时间才能有效地管理空闲内存。这种算法通常具有较高的内存使用效率（分配需要的内存），但是却需要花费更多时间来对堆进行管理。
另外一种方法称为buddy memory allocation，是一种更快的内存分配技术，它将内存划分为 2 的幂次方个分区，并使用 best-fit 方法来分配内存请求。当用户释放内存时，就会检查 buddy 块，查看其相邻的内存块是否也已经被释放。如果是的话，将合并内存块以最小化内存碎片。这个算法的时间效率更高，但是由于使用 best-fit 方法的缘故，会产生内存浪费。
slab 缓存 Linux 所使用的 slab 分配器的基础是 Jeff Bonwick 为 SunOS 操作系统首次引入的一种算法。Jeff 的分配器是围绕对象缓存进行的。在内核中，会为有限的对象集（例如文件描述符和其他常见结构）分配大量内存。Jeff 发现对内核中普通对象进行初始化所需的时间超过了对其进行分配和释放所需的时间。因此他的结论是不应该将内存释放回一个全局的内存池，而是将内存保持为针对特定目而初始化的状态。例如，如果内存被分配给了一个互斥锁，那么只需在为互斥锁首次分配内存时执行一次互斥锁初始化函数（mutex_init）即可。后续的内存分配不需要执行这个初始化函数，因为从上次释放和调用析构之后，它已经处于所需的状态中了。
Linux slab 分配器使用了这种思想和其他一些思想来构建一个在空间和时间上都具有高效性的内存分配器。
下图给出了 slab 结构的高层组织结构。在最高层是 cache_chain，这是一个 slab 缓存的链接列表。这对于 best-fit 算法非常有用，可以用来查找最适合所需要的分配大小的缓存（遍历列表）。cache_chain 的每个元素都是一个 kmem_cache 结构的引用（称为一个 cache）。它定义了一个要管理的给定大小的对象池。
https://www.ibm.com/developerworks/cn/linux/l-linux-slab-allocator/figure1.gif
每个缓存都包含了一个 slabs 列表，这是一段连续的内存块（通常都是页面）。存在 3 种 slab：
slabs_full 完全分配的 slab slabs_partial 部分分配的 slab slabs_empty 空 slab，或者没有对象被分配 注意 slabs_empty 列表中的 slab 是进行回收（reaping）的主要备选对象。正是通过此过程，slab 所使用的内存被返回给操作系统供其他用户使用。</description>
    </item>
    
    <item>
      <title>Centos上node彻底删除与安装并更新到最新版</title>
      <link>http://yezuozuo.github.io/program/centos%E4%B8%8Anode%E5%BD%BB%E5%BA%95%E5%88%A0%E9%99%A4%E4%B8%8E%E5%AE%89%E8%A3%85%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%88%B0%E6%9C%80%E6%96%B0%E7%89%88/</link>
      <pubDate>Fri, 03 Nov 2017 13:23:20 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/centos%E4%B8%8Anode%E5%BD%BB%E5%BA%95%E5%88%A0%E9%99%A4%E4%B8%8E%E5%AE%89%E8%A3%85%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%88%B0%E6%9C%80%E6%96%B0%E7%89%88/</guid>
      <description>删除 进入 /usr/local/lib 删除所有 node 和 node_modules文件夹
进入 /usr/local/include 删除所有 node 和 node_modules 文件夹
检查 ~ 文件夹里面的&amp;quot;local&amp;rdquo; &amp;ldquo;lib&amp;rdquo; &amp;ldquo;include&amp;rdquo; 文件夹，然后删除里面的所有 &amp;ldquo;node&amp;rdquo; 和 &amp;ldquo;node_modules&amp;rdquo; 文件夹
可以使用以下命令查找
find ~/ -name node find ~/ -name node_modules 进入 /usr/local/bin 删除 node 的可执行文件
以下步骤可选:
删除: /usr/local/bin/npm 删除: /usr/local/share/man/man1/node.1 删除: /usr/local/lib/dtrace/node.d 删除: rm -rf /home/[homedir]/.npm 删除: rm -rf /home/root/.npm 安装 方法一 下载 可将对应版本修改
wget https://nodejs.org/dist/v6.10.3/node-v6.10.3-linux-x64.tar.gz tar -zvxf node-v6.10.3-linux-x64.tar.gz 共享至全局 ln -s /path/node-v6.10.3/bin/node /usr/local/bin/node ln -s /path/node-v6.10.3/bin/npm /usr/local/bin/npm 方法二 准备 sudo yum -y install gcc make gcc-c++ openssl-devel wget 下载 可将对应版本修改</description>
    </item>
    
    <item>
      <title>php底层rtrim的一个“bug”</title>
      <link>http://yezuozuo.github.io/program/php%E5%BA%95%E5%B1%82rtrim%E7%9A%84%E4%B8%80%E4%B8%AAbug/</link>
      <pubDate>Thu, 19 Oct 2017 10:53:35 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E5%BA%95%E5%B1%82rtrim%E7%9A%84%E4%B8%80%E4%B8%AAbug/</guid>
      <description>背景 trim系列函数是用于去除字符串中首尾的空格或其他字符。ltrim函数只去除掉字符串首部的字符，rtrim函数只去除字符串尾部的字符。
string trim ( string $str [, string $character_mask = &amp;quot; \t\n\r\0\x0B&amp;quot; ] ) 看一个例子：
$str = &amp;quot;e?type&amp;quot;; echo $str; echo &amp;quot;\n&amp;quot;; echo rtrim($str, &#39;?type&#39;); echo &amp;quot;\n&amp;quot;; 不知道别人怎么想，我觉得这个返回的应该是”e”吧。
但是实际上返回了一个空的字符串。
又是一个黑魔法吗？
源码 看一下php的底层实现：
这个是php7 trim系列函数的源码，红框内就是rtrim的代码。
这个是php_charmask的源码。
执行步骤 trim执行步骤 trim、ltrim、rtrim三个函数都是调用了php_do_trim函数，区别在于第二个参数mode的不同。本文主要对trim函数进行分析，ltrim和rtrim函数跟trim的类似。然后php_do_trim会调用了php_trim来实现功能，因此trim函数的核心函数时php_trim函数。其执行步骤如下：
 根据what的值设置保存过滤字符的mask数组 过滤在字符串首部的待过滤字符 过滤在字符串尾部的待过滤字符  php_trim函数执行的流程图如下：
源码解读 php_trim函数先调用了php_charmask，这个函数试将过滤字符设置为mask[char] = 1的形式，这样就是一个哈希数组，然后可用于后面的判断。如果第二个参数是范围值时，调用了memset函数给mask数组赋值。
根据源码提炼出了以下的小demo:
#include &amp;lt;stdio.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;string.h&amp;gt; void php_charmask(unsigned char *input, size_t len, char *mask); char *rtrim(char *str,char *character_mask); int main(int argc, char const *argv[]) { printf(&amp;quot;%s\n&amp;quot;,rtrim(&amp;quot;e?</description>
    </item>
    
    <item>
      <title>让你的PHP7更快 GCC PGO</title>
      <link>http://yezuozuo.github.io/program/%E8%AE%A9%E4%BD%A0%E7%9A%84php7%E6%9B%B4%E5%BF%AB_gccpgo/</link>
      <pubDate>Mon, 11 Sep 2017 09:58:02 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E8%AE%A9%E4%BD%A0%E7%9A%84php7%E6%9B%B4%E5%BF%AB_gccpgo/</guid>
      <description>依据来源：让你的PHP7更快(GCC PGO) | 风雪之隅
执行步骤 下载最新的 php-7.2.0RC4.tar.gz https://downloads.php.net/~remi/php-7.2.0RC4.tar.gz
./configure --enable-fpm --with-config-file-path=/etc --with-config-file-scan-dir=/etc/php.d 第一遍编译PHP7, 让它生成会产生profile数据的可执行文件
make prof-gen -j 8 prof-gen参数是PHP7的Makefile特有的。
然后, 开始训练GCC:
需要在文件app.php中指定file和controller，不然会报404或者其他错误，这里的更改只是为了训练用，训练结束还需要将代码改回去。
$file = &#39;nearby_controller.php&#39;; $controller = &#39;nearby_controller&#39; 在nearby_controller.php中指定id和count，不然之后的代码无法训练到（尽量训练多的代码）。
进行训练，可以是100或者更多次，要注意看报错
sapi/cgi/php-cgi -T 100 /api_v2.php 训练结束，开始安装
make prof-clean make prof-use -j 8 sudo make install 压测结果 训练之前：
Requests per second: 326.02 [#/sec] (mean) 训练之后：
Requests per second: 361.31 [#/sec] (mean) 这个结果压测多次，发现有时训练之后的qps比训练之前的或高或低，但差距都不是很明显。
最终结论 GCC PGO在api的代码中优化效果不明显，而且操作起来较为复杂，不具备上线条件。
PGO相关解释 有编译器用到概率的，但我不知道最先进的编译器用到的概型有哪些先进的做法。编译器使用到概率的地方，最常见是跟profile-guided optimization（PGO）相关的。通过收集profile信息来估算某些代码执行的频繁程度，并对其做相应的优化。
例如说，
 cold code outlining  如果有：</description>
    </item>
    
    <item>
      <title>swoole 1.9.17 一个bug的处理过程</title>
      <link>http://yezuozuo.github.io/program/swoole-1.9.17-%E4%B8%80%E4%B8%AAbug%E7%9A%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B/</link>
      <pubDate>Wed, 02 Aug 2017 19:45:26 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/swoole-1.9.17-%E4%B8%80%E4%B8%AAbug%E7%9A%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B/</guid>
      <description>背景 在用最新版swoole 1.9.17 跑websocket服务时，在重启服务时worker进程会产生异常。
除了swoole版本不一样之外，在swoole1.9.13和2.0.7版本的环境中均未发现此问题。
而1.9.17版本刚刚重构了底层WorkerStop的机制：https://wiki.swoole.com/wiki/page/775.html
复现 issue详情：https://github.com/swoole/swoole-src/issues/1309
环境
websocket server端代码为官网的例子，设置worker_num为5。
运行server代码，查看进程
一个master进程，一个manager进程，5个worker进程，一切正常。
但是在多次重启时，kill -USR1 pid，会出现如下问题
然后查看进程数，发现进程数会增加。
排查过程 首先看多出的进程都是什么进程 多次对比，多出的都是worker进程，而且当发生上述错误的时候，就会多出进程，在之后重启时，该进程一直没有被kill掉。
咨询韩老师，回答：
 worker进程数量不一致是有可能的，新版本实现了异步安全reload，底层会先创建新的worker进程，因此统一时间可能会出现 worker_num * 2 数量的进程，老的 worker 进程会自然消亡并退出。
 但实际上多出的进程一直都没有被kill掉。
然后strace一下这些进程，首先是master进程，master进程一直只是event_loop和接收信号，没有异常。
看manager进程 发现在接收到USR1信号量的时候，对某些worker进程并没有全部发送信号量。
看有问题的worker进程 发现在master发送信号量的时候，该进程没有收到信号，而且在过一段时间接收到SIGTSTP信号的时候，该进程也没有退出。
解决 底层有BUG，异步安全reload 特性带来的，有一个特殊情况会出现，同时并发了2个信号。
解决代码如下：https://github.com/swoole/swoole-src/commit/9a79829cd0d16d1ce8af76a8e68fa444957d2992
完成 韩老师在1.9.18分支中fix了该问题，安装了该分支后重新测试上述场景：
已经没有上述问题。</description>
    </item>
    
    <item>
      <title>性能优化的一些思考</title>
      <link>http://yezuozuo.github.io/program/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</link>
      <pubDate>Mon, 03 Jul 2017 13:41:50 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</guid>
      <description>性能优化服务诉求
接口运行的怎么样？ 信息透明
接口调用时延为啥变高了？ 自动化诊断优化
接口刚优化了怎么又慢了？ 持续优化
上次的优化效果怎么样？ 量化跟踪，流程闭环
我们也想接入这个系统啊…… 产品化输出
这个自动化系统的悖论是我无意中看到的，在讲飞机的自动驾驶的时候，因为自动驾驶做的足够好，当出现紧急问题的时候，飞机驾驶员反而没有足够的能力去处理紧急的情况，这就是自动化系统的悖论。
技术体系的基本架构原则
 围绕业务发展。 结合团队规模和特点。 自动化、组件化、标准化。 聚焦效率、体验和质量。 如无必要，勿增实体。  </description>
    </item>
    
    <item>
      <title>深入浅出FastCGI和php Fpm</title>
      <link>http://yezuozuo.github.io/program/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAfastcgi%E5%92%8Cphp-fpm/</link>
      <pubDate>Mon, 05 Jun 2017 09:46:30 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAfastcgi%E5%92%8Cphp-fpm/</guid>
      <description>先看个图，标准的PHP单进程CLI和CGI生命周期。php进程启动，需要zend core、Module的Init(MINIT)、Request 的Init(RINIT) 这样的。一个进程只服务一次命令行或HTTP请求，就退出。
而FastCGI/php-fpm 就是改造后的多进程的 CGI，类似于资源池，预先启动 100个 php-fpm 进程，提前MINT，nginx的请求来了，直接进入 RINIT -&amp;gt; RSHUTDOWN 循环。请求结束，进程不退出，一个进程至少服务上万次请求才退出。
为什么一定要退出？怕RINIT-&amp;gt;RSHUTDOWN循环，有哪个代码写的不好，变量一直没释放，内存泄露GC又回收不了。php-fpm里的pm.max_requests配置就是设置RINT循环多少次，退出进程。
再来看几个 TSF、swoole、workerman、php-pm，都是 php 启动cli进程，用php管理子进程，php解析HTTP协议。
生命周期连RINIT-&amp;gt;MINIT循环都省了，没写在类属性里的变量，裸写的变量都是进程级全局变量，比php-fpm下的$_GET、$_POST、$_SERVER、$_SESSION、$_COOKIE这些全局变量范围还大，是进程级的。意味着你写了个a.php，里面定义了$a=1;赋值之后，下次请求过来，只要正好分配到了这个进程，依然还能取到普通定义的$a变量。
这意味着什么？像Laravel里的$app这些变量，只要写在最外面，因为没有触发RSHUTDOWN，又没有主动unset，GC引用计数器一直大于0，变量不会消失。
那怎么解决每次请求$_GET和$_POST不一样的问题？这些swoole、workerman进程管理器自己实现了小型化的INIT-&amp;gt;SHUTDOWN过程，维护一些引用计数呗，自己的a.php完成后，这种框架帮你unset($_GET)。
问题来了，稳定不稳定？swoole、workman框架本身稳定，但因为完全改变了php生命周期，业务开发人员不熟悉，一不小心写了global、static这样的变量，全局用了，内存越占越大，崩溃。又或者写了个exit，把整个进程exit而不是requestext了。</description>
    </item>
    
    <item>
      <title>php级别性能优化分享</title>
      <link>http://yezuozuo.github.io/program/php%E7%BA%A7%E5%88%AB%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%88%86%E4%BA%AB/</link>
      <pubDate>Wed, 17 May 2017 21:28:51 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E7%BA%A7%E5%88%AB%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%88%86%E4%BA%AB/</guid>
      <description>前言 指导思想：能自动化的，要自动化；不能自动化的，要半自动化。
性能分析最理想的方式是强智能，也就是让机器独立处理所有事情，自动发现问题并解决问题。但是需要用到人工智能，深度学习等等。要学的东西还很多，暂时还做不了那么牛逼的事，既然现在强智能还不够强，那么我们暂时先用弱智能+人工确认的方式，来实现「半智能化」：用机器帮咱们做预选，人工来做最终选择，虽然依然包含了人工干预，但却可以把生产效率提升几十倍。
性能分析  方法、工具、思路 查找瓶颈 重点优化（耗时多、请求多、超过150ms的接口） 查看单个请求调用链（次数，时间） 各个性能参数变化率 机器流量，负载，内存，qps，php.ini+php-fpm.conf  性能分析-工具 xhprof源码分析： https://zoco.fun/program/Xhprof源码分析
性能分析-方法  耗时 次数 聚合分析 增量分析 实时分析 数据报表 性能报表  耗时 原则：耗时是可控的
 找到耗时最多的调用：发现递归，循环，不合理的用法，异常的服务。 数据源调用耗时异常，去推动他们优化。 curl替代file_get_contents并且增加超时时间。 微服务的超时时间在合理的范围内尽量小一些。  次数 原则：越少越好
 找到调用次数异常的调用：寻找批量方案或者预加载方案。 对多个无上下关联的数据源并行化或者异步化。  聚合分析  能看到当前接口所有运行过的函数。 平均值处理，也避免了程序运行中的不规律事件。 统计值比孤立值更具说服力。 为增量统计做准备。  增量分析 检测每个接口多余出的方法以及此类方法的耗时从而得出业务变化对性能的影响。（对新上的代码）
实时分析 因为数据量相对较少，所以每小时跑一次数据，暂时针对各个数据源整体耗时和该接口所有微服务进行聚合。
如果有全量的日志，那么这个做起来就很有意义了。
数据报表 项目性能分析总览，调用次数，总耗时，平均耗时，趋势。
 单日性能统计，天级别（次数、平均耗时、最大耗时、平均内存、最大内存） 所有uri性能总览，uri级别（uri、次数、平均耗时、最大耗时、平均内存、最大内存、微服务(Max)、Redis(Max)、Mongo(Max)） 单个uri性能分析（时间、耗时、内存、微服务、Redis、Mongo、Host） 单个uri性能整合分析（耗时分布饼图、整体耗时、耗时分布面积图、耗时折线图、各功能耗时、实时数据） xhprof html xhprof graph  性能报表 这里是根据已有的数据进行二次分析
 邮件汇总接口图表（每日邮件数据汇总） 超出150ms的接口（重点分析） 所有接口分析（接口内的方法聚合） 微服务分析（每个接口调用了哪些微服务） 微服务排行榜（所有微服务调用的耗时和次数）  性能分析－优化方案  预加载 批量 短路 降维 并行和异步化 抽象 缓存  预加载 事先将请求到的数据放到内存中，同一次请求同一份数据不要取两次，用的时候随时取（bridge）。</description>
    </item>
    
    <item>
      <title>如何进行一次简单的性能分析</title>
      <link>http://yezuozuo.github.io/program/%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E4%B8%80%E6%AC%A1%E7%AE%80%E5%8D%95%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</link>
      <pubDate>Mon, 03 Apr 2017 20:29:42 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E4%B8%80%E6%AC%A1%E7%AE%80%E5%8D%95%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</guid>
      <description>基于xhprof开发php性能优化系统全文
安装 安装php msgpack扩展 安装php xhprof扩展 php5 https://github.com/phacility/xhprof
php7 https://github.com/Yaoguais/phpng-xhprof
安装supervisor 打桩 这一步是将xhprof产生的数据存入redis中。
在程序入口文件，比如index.php中添加下列代码：
// xhprof打点频率 define(&#39;XHPROF_FREQUENCY&#39;,1000); if ((mt_rand(1, XHPROF_FREQUENCY) === 1) &amp;amp;&amp;amp; function_exists(&#39;xhprof_enable&#39;)) { xhprof_enable(XHPROF_FLAGS_MEMORY|XHPROF_FLAGS_CPU); } register_shutdown_function(function () { $uname = php_uname(&#39;n&#39;); $time = time(); $xhprofData = xhprof_disable(); $log = array( &#39;REQUEST_URI&#39; =&amp;gt; parse_url($_SERVER[&#39;REQUEST_URI&#39;], PHP_URL_PATH), &#39;HTTP_HOST&#39;	=&amp;gt; $uname, &#39;REQUEST_METHOD&#39; =&amp;gt; $_SERVER[&#39;REQUEST_METHOD&#39;], &#39;REQUEST_TIME&#39; =&amp;gt; $time, &#39;xhprof_data&#39; =&amp;gt; $xhprofData, ); // 压缩日志 $log = msgpack_pack($log); $key = &#39;chan-xhprof-log&#39;; $redis = new Redis(); $redis-&amp;gt;connect(&#39;127.</description>
    </item>
    
    <item>
      <title>程序的本质复杂性和元语言抽象</title>
      <link>http://yezuozuo.github.io/program/%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%AC%E8%B4%A8%E5%A4%8D%E6%9D%82%E6%80%A7%E5%92%8C%E5%85%83%E8%AF%AD%E8%A8%80%E6%8A%BD%E8%B1%A1/</link>
      <pubDate>Wed, 01 Mar 2017 20:26:20 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%AC%E8%B4%A8%E5%A4%8D%E6%9D%82%E6%80%A7%E5%92%8C%E5%85%83%E8%AF%AD%E8%A8%80%E6%8A%BD%E8%B1%A1/</guid>
      <description>参考： http://www.cnblogs.com/weidagang2046/p/the-nature-of-meta.html
 如果目标还是代码“简短、优雅、易理解、易维护”，那么代码优化是否有一个理论极限？这个极限是由什么决定的？普通代码比起最优代码多出来的“冗余部分”到底干了些什么事情？
 回答这个问题要从程序的本质说起。Pascal语言之父Niklaus Wirth在70年代提出：
Program = Data Structure + Algorithm 随后逻辑学家和计算机科学家R Kowalski进一步提出：
Algorithm = Logic + Control 谁更深刻更有启发性？当然是后者！而且我认为数据结构和算法都属于控制策略，程序包含了逻辑和控制两个维度。
逻辑就是问题的定义，比如，对于排序问题来讲，逻辑就是“什么叫做有序，什么叫大于，什么叫小于，什么叫相等”？控制就是如何合理地安排时间和空间资源去实现逻辑。逻辑是程序的灵魂，它定义了程序的本质；控制是为逻辑服务的，是非本质的，可以变化的，如同排序有几十种不同的方法，时间空间效率各不相同，可以根据需要采用不同的实现。
程序的复杂性包含了本质复杂性和非本质复杂性两个方面。套用这里的术语， 程序的本质复杂性就是逻辑，非本质复杂性就是控制。逻辑决定了代码复杂性的下限，也就是说不管怎么做代码优化，Office程序永远比Notepad程序复杂，这是因为前者的逻辑就更为复杂。如果要代码简洁优雅，任何语言和技术所能做的只是尽量接近这个本质复杂性，而不可能超越这个理论下限。
理解&amp;quot;程序的本质复杂性是由逻辑决定的&amp;quot;从理论上为我们指明了代码优化的方向：让逻辑和控制这两个维度保持正交关系。来看Java的Collections.sort方法的例子：
interface Comparator&amp;lt;T&amp;gt; { int compare(T o1, T o2); } public static &amp;lt;T&amp;gt; void sort(List&amp;lt;T&amp;gt; list, Comparator&amp;lt;? super T&amp;gt; comparator) 使用者只关心逻辑部份，即提供一个Comparator对象表明序在类型T上的定义；控制的部分完全交给方法实现者，可以有多种不同的实现，这就是逻辑和控制解耦。同时，我们也可以断定，这个设计已经达到了代码优化的理论极限，不会有比本质上比它更简洁的设计（忽略相同语义的语法差异），为什么呢？因为逻辑决定了它的本质复杂度，Comparator和Collections.sort的定义完全是逻辑的体现，不包含任何非本质的控制部分。
另外需要强调的是，上面讲的“控制是非本质复杂性”并不是说控制不重要，控制往往直接决定了程序的性能，当我们因为性能等原因必须采用某种控制的时候，实际上被固化的控制策略也是一种逻辑。比如，当你的需求是“从进程虚拟地址ptr1拷贝1024个字节到地址ptr2“，那么它就是问题的定义，它就是逻辑，这时，提供进程虚拟地址直接访问语义的底层语言就与之完全匹配，反而是更高层次的语言对这个需求无能为力。
介绍了逻辑和控制的关系，可能很多朋友已经开始意识到了上面二进制文件解析实现的问题在哪里，其实这也是 绝大多数程序不够简洁优雅的根本原因：逻辑与控制耦合。上面那个消息定义表格就是不包含控制的纯逻辑，我相信即使不是程序员也能读懂它；而相应的代码把逻辑和控制搅在一起之后就不那么容易读懂了。
元抽象表达 思考一个问题：
 逻辑决定了程序的本质复杂性，但接口不是表达逻辑的通用方式，那么是否存在表达逻辑的通用方式呢？
 答案是：有！这就是元(Meta)，包括元语言(Meta Language)和元数据(Meta Data)两个方面。元并不神秘，我们通常所说的配置就是元，元语言就是配置的语法和语义，元数据就是具体的配置，它们之间的关系就是C语言和C程序之间的关系；但是，同时元又非常神奇，因为元既是数据也是代码，在表达逻辑和语义方面具有无与伦比的灵活性。至此，我们终于找到了让代码变得简洁、优雅、易理解、易维护的终极方法，这就是： 通过元语言抽象让逻辑和控制彻底解耦！
比如，对于二进制消息解析，经典的做法是类似Google的Protocol Buffers，把消息结构特征抽象出来，定义消息描述元语言，再通过元数据描述消息结构。下面是Protocol Buffers元数据的例子，这个元数据是纯逻辑的表达，它的复杂度体现的是消息结构的本质复杂度，而如何序列化和解析这些控制相关的部分被Protocol Buffers编译器隐藏起来了。
message Person { required int32 id = 1; required string name = 2; optional string email = 3; } 元语言解决了逻辑表达问题，但是最终要与控制相结合成为具体实现，这就是元语言到目标语言的映射问题。通常有这两种方法：</description>
    </item>
    
    <item>
      <title>php-fpm的一些实验</title>
      <link>http://yezuozuo.github.io/program/php-fpm%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E9%AA%8C/</link>
      <pubDate>Tue, 07 Feb 2017 19:37:03 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php-fpm%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E9%AA%8C/</guid>
      <description>实验一  将pm设置成static方式 将pm.max_children设置为1 将pm.max_requests设置成500  每次请求，程序末尾包括exit()和不包括exit()的形式，php-fpm的子进程都不会被杀死。
实验二  将pm设置成static方式 将pm.max_children设置为1 将pm.max_requests设置成1  每次请求，程序末尾包括exit()和不包括exit()的形式，php-fpm的子进程都会被杀死。
实验三 &amp;lt;?php class a { public $a; public function __construct() { $this-&amp;gt;a = 1; exit(); } public function __destruct() { echo $this-&amp;gt;a.&amp;quot;\n&amp;quot;; echo &#39;123&#39;.&amp;quot;\n&amp;quot;; } } $a = new a(); echo $a-&amp;gt;a; 输出
1 123 结论：php中的exit()并不能使这个进程直接结束。尽管调用了exit(),Shutdown函数以及object destructors总是会被执行。</description>
    </item>
    
    <item>
      <title>Zabbix怎么搞一个永不过期的session</title>
      <link>http://yezuozuo.github.io/program/zabbix%E6%80%8E%E4%B9%88%E6%90%9E%E4%B8%80%E4%B8%AA%E6%B0%B8%E4%B8%8D%E8%BF%87%E6%9C%9F%E7%9A%84session/</link>
      <pubDate>Thu, 19 Jan 2017 20:33:28 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/zabbix%E6%80%8E%E4%B9%88%E6%90%9E%E4%B8%80%E4%B8%AA%E6%B0%B8%E4%B8%8D%E8%BF%87%E6%9C%9F%E7%9A%84session/</guid>
      <description>zabbix的session会隔一段时间就不可用了，查一下原因
zabbix的session什么时候过期？ 看zabbix的数据库，session相关的表有两个：
mysql&amp;gt; desc users; +----------------+---------------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------------+---------------------+------+-----+---------+-------+ | userid | bigint(20) unsigned | NO | PRI | NULL | | | alias | varchar(100) | NO | MUL | | | | name | varchar(100) | NO | | | | | surname | varchar(100) | NO | | | | | passwd | char(32) | NO | | | | | url | varchar(255) | NO | | | | | autologin | int(11) | NO | | 0 | | | autologout | int(11) | NO | | 900 | | | lang | varchar(5) | NO | | en_GB | | | refresh | int(11) | NO | | 30 | | | type | int(11) | NO | | 0 | | | theme | varchar(128) | NO | | default | | | attempt_failed | int(11) | NO | | 0 | | | attempt_ip | varchar(39) | NO | | | | | attempt_clock | int(11) | NO | | 0 | | | rows_per_page | int(11) | NO | | 50 | | +----------------+---------------------+------+-----+---------+-------+ mysql&amp;gt; desc sessions; +------------+---------------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +------------+---------------------+------+-----+---------+-------+ | sessionid | varchar(32) | NO | PRI | | | | userid | bigint(20) unsigned | NO | MUL | NULL | | | lastaccess | int(11) | NO | | 0 | | | status | int(11) | NO | | 0 | | +------------+---------------------+------+-----+---------+-------+ 登录时验证sql为</description>
    </item>
    
    <item>
      <title>Swoole笔记</title>
      <link>http://yezuozuo.github.io/program/swoole%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Thu, 15 Dec 2016 10:00:29 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/swoole%E7%AC%94%E8%AE%B0/</guid>
      <description>&amp;lt;1&amp;gt;  manager是进程，worker是进程，task是进程，master是线程，reactor是线程，心跳检测是线程，UDP收发是线程 reactor和worker之间的通信是通过IPC实现的 和worker进行通信有两种方式：管道和消息队列 主进程mainReactor负责监听server socket tcp分为nopush和nodelay两种方式 主进程mainReactor:  负责监听server socket，评估每个reactor线程的连接数量，评估方式也就是分配的方式是通过fd%serv-&amp;gt;reactorNum实现的 将监听到的accept请求分配给连接数最少的reactor线程 接管所有信号的signal处理，使reactor线程运行中不受到打扰   管理进程manager  分为worker进程和taskWorker进程 所有的worker进程和task进程都是manager进程fork出来的 当worker进程发生致命错误或者运行生命周期结束时，管理进程会回收此进程，并创建新的进程，防止子进程成为僵尸进程 管理进程可以平滑重启所有worker进程，以实现程序代码的重新加载   异步reactor线程（全异步非阻塞）  收发数据，处理网络I/O 处理TCP连接，将发来的数据缓冲拼接，拆分成完整的请求包 负责监听从mainReactor分配来的socket socket可读时读取数据，进行协议解析，然后将数据投递到worker进程，在socket可读时将数据发给TCP客户端   同步或者异步worker进程，没有用到epoll  处理数据 接受由reactor线程投递的请求数据包，并执行PHP回调函数处理数据 生成响应数据并发给reactor线程，由reactor线程发给客户端   task worker进程（完全是同步阻塞模式）  有些逻辑代码不需要马上执行，可以将一个task任务投递到taskworker进程池，在worker进程空闲时再去捕获任务执行的结果。   factory&amp;lt;-&amp;gt;task  不生产实例 根据类型的不同执行 任务中心，一个task请求进入factory，会通过dispatch分配，onTask处理，onFinish交付结果一系列流程 FactoryProcess用于管理manager和worker进程，也会对单独的writer线程管理   如果reactor最大允许监听的事件数比reactor的事件数小的话用poll/select，否则用epoll/kqueue client的类型：TCP，TCP6，UDP，UDP6，UNIXSTREAM，UNIXDGRAM client  异步：socket从mainReactor中获取 同步：直接创建一个connection   DNS：只可以是异步的查询，hashMap  &amp;lt;2&amp;gt; 腾讯QQ也是有C10K问题的，只不过他们是用了UDP这种原始的包交换协议来实现的，绕开了这个难题。当然过程肯定是痛苦的。如果当时有epoll技术，他们肯定会用TCP。后来的手机QQ，微信都采用TCP协议。实际上当时也有异步模式，如：select/poll模型，这些技术都有一定的缺点，如selelct最大不能超过1024，poll没有限制，但每次收到数据需要遍历每一个连接查看哪个连接有数据请求。既然有了C10K问题，程序员们就开始行动去解决它。于是FreeBSD推出了kqueue，Linux推出了epoll，Windows推出了IOCP。这些操作系统提供的功能就是为了解决C10K问题。因为Linux是互联网企业中使用率最高的操作系统，Epoll就成为C10K killer、高并发、高性能、异步非阻塞这些技术的代名词了。
epoll技术的编程模型就是异步非阻塞回调，也可以叫做Reactor，事件驱动，事件轮循（EventLoop）。Epoll就是为了解决C10K问题而生。使用Epoll技术，使得小公司也可以玩高并发。不需要购买很多服务器，有几台服务器就可以服务大量用户。Nginx，libevent，node.js这些就是Epoll时代的产物。
协程的优点是它比系统线程开销小，缺点是如果其中一个协程中有密集计算，其他的协程就不运行了。操作系统进程的缺点是开销大，优点是无论代码怎么写，所有进程都可以并发运行。
Erlang解决了协程密集计算的问题，它基于自行开发VM，并不执行机器码。即使存在密集计算的场景，VM发现某个协程执行时间过长，也可以进行中止切换。Golang由于是直接执行机器码的，所以无法解决此问题。所以Golang要求用户必须在密集计算的代码中，自行Yield。
实际上同步阻塞程序的性能并不差，它的效率很高，不会浪费资源。当进程发生阻塞后，操作系统会将它挂起，不会分配CPU。直到数据到达才会分配CPU。多进程只是开多了之后副作用太大，因为进程多了互相切换有开销。所以如果一个服务器程序只有1000左右的并发连接，同步阻塞模式是最好的。</description>
    </item>
    
    <item>
      <title>教你一步一步写一个phpunit testcase</title>
      <link>http://yezuozuo.github.io/program/%E6%95%99%E4%BD%A0%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E5%86%99%E4%B8%80%E4%B8%AAphpunit-testcase/</link>
      <pubDate>Sat, 05 Nov 2016 10:53:35 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E6%95%99%E4%BD%A0%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E5%86%99%E4%B8%80%E4%B8%AAphpunit-testcase/</guid>
      <description>程序地址 https://github.com/yezuozuo/how-to-write-a-phpunit-testcase
使用方法  composer install phpunit tests/EventTest.php  背景 https://phpunit.de/manual/current/zh_cn/index.html 这个是phpunit的文档,但是看完了我完全不知道怎么入手好吗,从0到1这个过程就在下面。
综述 目录结构
. |-- reports |-- src | -- PHPUnitEventDemo | -- Event.php | -- EventException.php | -- User.php |-- tests | -- EventTest.php |-- .gitignore |-- composer.json |-- phpunit.xml |-- README.md  PHPUnitEventDemo - 下面是要测试的类 * Event.php - Event类 * EventException.php - Event异常类 * User.php - User类 tests - 单元测试目录 * EventTest.php - 测试Event类的测试用例  Assertions（断言） 断言为PHPUnit的主要功能，用来验证单元的执行结果是不是预期值。</description>
    </item>
    
    <item>
      <title>谈消息安全传输中的技术点</title>
      <link>http://yezuozuo.github.io/program/%E8%B0%88%E6%B6%88%E6%81%AF%E5%AE%89%E5%85%A8%E4%BC%A0%E8%BE%93%E4%B8%AD%E7%9A%84%E6%8A%80%E6%9C%AF%E7%82%B9/</link>
      <pubDate>Sat, 29 Oct 2016 14:04:02 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E8%B0%88%E6%B6%88%E6%81%AF%E5%AE%89%E5%85%A8%E4%BC%A0%E8%BE%93%E4%B8%AD%E7%9A%84%E6%8A%80%E6%9C%AF%E7%82%B9/</guid>
      <description>一、初级阶段：信息裸传 特点：在网络上传递明文
黑客定理一：网络上传递的数据是不安全的，属网络于黑客公共场所，能被截取。
结果：传递明文无异于不穿衣服裸奔
改进方案：先加密，再在网络上传输
二、进阶阶段：传输密文 特点：
 服务端和客户端先约定好加密算法，加密密钥 客户端，传输前用约定好的密钥加密 传输密文 服务端，收到消息后用约定好的密钥解密  这么传输消息安全么？
黑客定理二：客户端的代码是不安全的，属于黑客本地范畴，能被逆向工程，任何客户端与服务端提前约定好的算法与密钥都是不安全的
结果：任何客户端的代码混淆，二进制化都只能提高黑客的破解门槛，本质是不安全的
改进方案：不能固定密钥
三、中级阶段：服务端为每个用户生成密钥 特点：
 客户端和服务端提前约定好加密算法，在传递消息前，先协商密钥 客户端，请求密钥 服务端，返回密钥 然后用协商密钥加密消息，传输密文  这么传输安全么？
结果：
 如黑客定理一，网上传输的内容是不安全的，于是乎，黑客能得到加密key=X 如黑客定理二，客户端和服务端提前约定的加密算法是不安全的，于是乎，黑客能得到加密算法 于是乎，黑客截取后续传递的密文，可以用对应的算法和密钥解密  改进方案：协商的密钥不能在网络上传递
四、再进阶阶段：客户端确定密钥，密钥不再传输 特点：
 协商的密钥无需在网络传输 使用“具备用户特性的东西”作为加密密钥，例如：用户密码的散列值 一人一密，每个人的密钥不同 然后密钥加密消息，传输密文 服务端从db里获取这个“具备用户特性的东西”，解密  这么传输安全么？
黑客定理三：用户客户端内存是安全的，属于黑客远端范畴，不能被破解
当然，用户中了木马，用户的机器被控制的情况不在此列，如果机器真被控制，监控用户屏幕就好了，就不用搞得这么麻烦了
结果：使用“具备用户特性的东西”作为加密密钥，一人一密，是安全的。只是，当“具备用户特性的东西”泄漏，就有潜在风险
五、高级阶段：一次一密，密钥协商 特点：每次通信前，进行密钥协商，一次一密
密钥协商过程，如下图所述，需要随机生成三次密钥，两次非对称加密密钥（公钥，私钥），一次对称加密密钥，简称安全信道建立的“三次握手”，在客户端发起安全信道建立请求后：
 服务端随机生成公私钥对(公钥pk1，私钥pk2)，并将公钥pk1传给客户端  (注意：此时黑客能截获pk1)
 客户端随机生成公私钥对(公钥pk11，私钥pk22)，并将公钥pk22，通过pk1加密，传给服务端  (注意：此时黑客能截获密文，也知道是通过pk1加密的，但由于黑客不知道私钥pk2，是无法解密的)
服务端收到密文，用私钥pk2解密，得到pk11
 服务端随机生成对称加密密钥key=X，用pk11加密，传给客户端  (注意：同理，黑客由密文无法解密出key)
客户端收到密文，用私钥pk22解密，可到key=X
至此，安全信道建立完毕，后续通讯用key=X加密，以保证信息的安全性
六、总结 黑客定理一：网络上传递的数据是不安全的，属于黑客公共场所，能被截取
黑客定理二：客户端的代码是不安全的，属于黑客本地范畴，能被逆向工程，任何客户端与服务端提前约定好的算法与密钥都是不安全的
黑客定理三：用户客户端内存是安全的，属于黑客远端范畴，不能被破解
对于不同加密方法明：
 明文消息传递如同裸奔，不安全 客户端和服务端提前约定加密算法和密钥，不安全（好多公司都是这么实现的=_=） 服务端随机生成密钥，发送给客户端，不安全 一人一密，客户端使用“具备用户特性的东西”作为加密密钥，弱安全 一次一密，三次握手建立安全信道，安全  只要即时通讯公司有良知，不从服务端偷看，一切都是安全的。额，这个“只要”的假设，貌似不成立</description>
    </item>
    
    <item>
      <title>Nginx&#43;php Fpm模式php内存泄漏探究</title>
      <link>http://yezuozuo.github.io/program/nginx&#43;php-fpm%E6%A8%A1%E5%BC%8Fphp%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%8E%A2%E7%A9%B6/</link>
      <pubDate>Fri, 23 Sep 2016 19:50:52 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/nginx&#43;php-fpm%E6%A8%A1%E5%BC%8Fphp%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%8E%A2%E7%A9%B6/</guid>
      <description>昨天遇到过一次服务器内存告警，查看后发现有个php-fpm进程占用了2G的内存。但我明明在php.ini文件里面，有配置 memory_limit = 256M，那为什么会有占用2G内存的php-fpm进程呢？
这里先简单说一下nginx+php-fpm模式的工作原理。  nginx服务器fork出n个子进程（worker），php-fpm管理器fork出n个子进程。 当有用户请求，nginx的一个worker接收请求，并将请求抛到socket中。 php-fpm空闲的子进程监听到socket中有请求，接收并处理请求。  这里要重点说一下第三步骤。第三步涉及到php-fpm进程生命周期的东西。一个php-fpm的生命周期大致是这样的：模块初始化（MINIT）-&amp;gt; 模块激活（RINIT）-&amp;gt; 请求处理 -&amp;gt; 模块停用（RSHUTDOWN） -&amp;gt; 模块激活（RINIT）-&amp;gt; 请求处理 -&amp;gt; 模块停用（RSHUTDOWN）……. 模块激活（RINIT）-&amp;gt; 请求处理 -&amp;gt; 模块停用（RSHUTDOWN）-&amp;gt; 模块关闭（MSHUTDOWN）。在一个php-fpm进程的生命周期里，会有多次的模块激活（RINIT）-&amp;gt; 请求处理 -&amp;gt; 模块停用（RSHUTDOWN）的过程。这个“请求处理”的大致过程是这样的：php读取相应的php文件，对其进行词法分析，生成opcode，zend虚拟机执行opcode。
回到一开始说的PHP配置文件里面的memory_limit 这个东西，其实，它限制的只是这个“请求处理”的内存。所以，这个参数跟php-fpm进程占用的内存并没有什么关系。那为什么会有占用2G大小的php-fpm进程呢？原因是这样的：php是用c写的，所以，难免又会一些内存泄露。也就是说，在“请求处理”这个过程结束后，有些变量没有被销毁，然后就导致一个php-fpm进程占用的内存越来越大。
那么，有什么办法能阻止这个问题呢？方法一：写不泄漏内存的php程序；方法二：在php-fpm配置文件中，将pm.max_requests这个参数设置小一点。这个参数的含义是：一个php-fpm子进程最多处理pm.max_requests个用户请求后，就会被销毁。当一个php-fpm进程被销毁后，它所占用的所有内存都会被回收。</description>
    </item>
    
    <item>
      <title>动态流量控制（滑动窗口）在api日志中的应用</title>
      <link>http://yezuozuo.github.io/program/%E5%8A%A8%E6%80%81%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E5%9C%A8api%E6%97%A5%E5%BF%97%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</link>
      <pubDate>Sat, 18 Jun 2016 18:33:01 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E5%8A%A8%E6%80%81%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E5%9C%A8api%E6%97%A5%E5%BF%97%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</guid>
      <description>现在api的所有php-fpm日志都打到了elasticsearch里，我们可以拿这些日志来做很多东西，但首先第一步是怎么取这些数据。
这些日志的量大概每天有几百万条的规模。
首先我尝试的是通过from+size的方式取数据，但是elasticsearch默认情况下只可以取from+size&amp;lt;10000的数据（当然也可以调大from+size，但是会带来一个问题就是数据取到最后越来越慢）
 Result window is too large, from + size must be less than or equal to: [10000] but was [10020]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter
 那么就采取第二个方案，就是按时间来取数据。
那么问题来了，按多久取一次数据呢？
之前的取数据一般是按分钟取，但是会遇到一个问题。
先看下面的图，是redis_warning一天（20170110）的分布情况：
从图里我们可以看到两个特点：
 请求包括报错的分布都是有高峰的。 正常情况下数据的涨跌都不是跳崖式的波动，都是有一个涨跌的过渡过程（当然突然某个redis或者moa挂掉会导致一个突发的增长，这种情况也应该考虑到）。  我们的请求包括报错的分布都是有高峰的，所以说在高峰的时候按分钟取也可能出现from+size&amp;gt;10000的情况，那么我们就来按秒取，按秒取的话一天相当于要请求elasticsearch的接口86400次，在非高峰的时候这样无疑是对带宽的一种浪费。而且执行一次脚本所需要的时间很长，测试和出数据的的时候极其不方便（redis_warning按秒执行一次的时间大概是90分钟）。
下面就是我现在所采用的方案，就是动态流量控制（滑动窗口）。
既然非高峰的时候我可以多取一段时间的数据，非高峰的时候少取一些数据，那么我就建立一个模型让脚本动态地根据上一次获得的数据来决定下一次请求所要获取的数据多少。
if(count($res) == 10000) { //说明此次请求获取的数据超过阈值 //丢弃掉此次请求的数据 //将下一次请求的时间间隔/2（滑动窗口缩小） } else if (count($res) &amp;lt; 2000) { //说明此次请求的数据太少 //保留此次数据 //将下一次请求的时间间隔*2（滑动窗口扩大） } 上面例子中的2000是可以调整的。</description>
    </item>
    
    <item>
      <title>案例学习：仅使用redis&#43;php设计实现一个简单的Twitter</title>
      <link>http://yezuozuo.github.io/program/%E6%A1%88%E4%BE%8B%E5%AD%A6%E4%B9%A0%E4%BB%85%E4%BD%BF%E7%94%A8redis&#43;php%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84twitter/</link>
      <pubDate>Mon, 30 May 2016 18:50:41 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E6%A1%88%E4%BE%8B%E5%AD%A6%E4%B9%A0%E4%BB%85%E4%BD%BF%E7%94%A8redis&#43;php%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84twitter/</guid>
      <description>参考： http://redis.io/topics/twitter-clone
代码地址： https://github.com/yezuozuo/twitter_clone
我会在此文中描述如何使用PHP以及仅使用Redis来设计实现一个简单的Twitter克隆。很多编程社区常认为KV储存是一个特别的数据库，在web应用中不能替代关系数据库。本文尝试证明这恰恰相反。
Key-value 数据库基础 KV数据的精髓，是能够把value储存在key里，此后该数据仅能够通过确切的key来获取，无法搜索一个值。确切的来讲，它更像一个大型HASH/字典，但它是持久化的，比如，当你的程序终止运行，数据不会消失。 比如我们能用SET命令以key foo 来储存值 bar
SET foo bar Redis会永久储存我们的数据，所以之后我们可以问Redis：“储存在key foo里的数据是什么？”，Redis会返回一个值：bar
GET foo =&amp;gt; bar KV数据库提供的其他常见操作有:DEL，用于删除指定的key和关联的value； SET-if-not-exists (在Redis上称为SETNX )仅会在key不存在的时候设置一个值； INCR能够对指定的key里储存的数字进行自增。
 SET foo 10 INCR foo =&amp;gt; 11 INCR foo =&amp;gt; 12 INCR foo =&amp;gt; 13 原子操作 目前为止它是相当简单的，但是INCR有些不同。设想一下，为什么要提供这个操作？毕竟我们自己能用以下简单的命令实现这个功能： x = GET foo x = x + 1 SET foo x 问题在于要使上面的操作正常进行，同时只能有一个客户端操作x的值。看看如果两台电脑同时操作这个值会发生什么： x = GET foo (返回10) y = GET foo (返回10) x = x + 1 (x现在是11) y = y + 1 (y现在是11) SET foo x (foo现在是11) SET foo y (foo现在是11) 问题发生了！我们增加了值两次，本应该从10变成12，现在却停留在了11。这是因为用GET和SET来实现INCR不是一个原子操作(atomic operation)。所以Redis\memcached之类提供了一个原子的INCR命令，服务器会保护get-increment-set操作，以防止同时的操作。让Redis与众不同的是它提供了更多类似INCR的方案，用于解决模型复杂的问题。因此你可以不使用任何SQL数据库、仅用Redis写一个完整的web应用，而不至于抓狂。</description>
    </item>
    
    <item>
      <title>《七周七数据库》</title>
      <link>http://yezuozuo.github.io/program/%E4%B8%83%E5%91%A8%E4%B8%83%E6%95%B0%E6%8D%AE%E5%BA%93/</link>
      <pubDate>Mon, 30 May 2016 13:43:45 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E4%B8%83%E5%91%A8%E4%B8%83%E6%95%B0%E6%8D%AE%E5%BA%93/</guid>
      <description></description>
    </item>
    
    <item>
      <title>php语法性能比较</title>
      <link>http://yezuozuo.github.io/program/php%E8%AF%AD%E6%B3%95%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83/</link>
      <pubDate>Fri, 29 Apr 2016 21:24:46 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E8%AF%AD%E6%B3%95%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83/</guid>
      <description>Php近似语法的性能分析
100000次 只是在语法上考虑性能 实际情况应该考虑可读性等问题综合使用 详细代码在 https://github.com/yezuozuo/php-optim
1.@ @test(); 0.10025715827942 s test(); 0.09039306640625 s 2.deep array $arr[1][2][3][4][5][6][7] = $i; 0.037128925323486 s $arr2[1] = $i; 0.018270969390869 s 3.defined var $a = null; $a = 1; 0.011500120162964 s $b = 1; 0.010693073272705 s 4.print vs echo print($strings); 1.756313085556 s echo $strings; 1.4546310901642 s 5.== vs === if (null == $n) {} 0.015053033828735 s if (null === $n) {} 0.013232946395874 s 6.null vs is_null if (is_null($n)) {} 0.</description>
    </item>
    
    <item>
      <title>php output buffer</title>
      <link>http://yezuozuo.github.io/program/php-output-buffer/</link>
      <pubDate>Wed, 23 Mar 2016 19:50:52 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php-output-buffer/</guid>
      <description>LNMP架构中的输出缓冲区 首先来看一下LNMP架构中涉及输出缓冲区的部分，后面会依次验证：
从图中，可以看出 Nginx层，SAPI层(PHP-FPM)和PHP内核中均有输出缓冲区，由此可见缓冲区是多么重要的一部分。
PHP-FPM 输出缓冲区 探讨下PHP-FPM 中的输出缓冲区，先来设置一个ini配置: output_buffering，把他设置为 0：
; Development Value: 4096 ; Production Value: 4096 ; http://php.net/output-buffering output_buffering = 0 设置完这个参数后，便关闭了PHP内核中的默认缓冲区。 来看一段代码，因为nginx中设置的fastcgi buffer的大小是4k，而每次输出的数据都是大于4k的，因此输出不会受到nginx输出缓冲区的影响。
未使用任何flush函数 &amp;lt;?php /** * 默认页面 */ class IndexController extends AbstractController { public function process() { for($i = 0; $i &amp;lt; 10; ++$i) { echo $i; echo str_repeat(&#39; &#39;, 4096) . &amp;quot;&amp;lt;br /&amp;gt;&amp;quot;; sleep(1); } exit; } } 运行这段代码，预期是每次输出1个数字，可是结果却是每次输出2个数字，即0，1一起输出，2，3一起输出，这是为什么呢？ 由于关闭了PHP内核中的输出缓冲，也没有使用ob_start系列的函数，而且数据量也大于了nginx中的输出缓冲，所以我猜测PHP-FPM中还有一个输出缓冲区,通过查阅fpm的源代码(php-5.6.10/sapi/fpm/fpm/fastcgi.h文件)，我发现在fcgi_request结构中有一个out_buf参数，大小是8k：
typedef struct _fcgi_request { int listen_socket; #ifdef _WIN32 int tcp; #endif int fd; int id; int keep; int closed; int in_len; int in_pad; fcgi_header *out_hdr; unsigned char *out_pos; unsigned char out_buf[1024*8]; unsigned char reserved[sizeof(fcgi_end_request_rec)]; HashTable *env; } fcgi_request; 于是我觉得这个参数应该就是PHP-FPM中的输出缓冲区大小。 OK，来验证一下，将上面代码中空格的数目改为8192试试，即：</description>
    </item>
    
    <item>
      <title>《Linux高性能服务器编程》</title>
      <link>http://yezuozuo.github.io/program/linux%E9%AB%98%E6%80%A7%E8%83%BD%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BC%96%E7%A8%8B/</link>
      <pubDate>Mon, 21 Mar 2016 13:44:46 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/linux%E9%AB%98%E6%80%A7%E8%83%BD%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BC%96%E7%A8%8B/</guid>
      <description>使用“池”以牺牲空间换取效率，使用零拷贝函数以避免内核和用户空间的切换等 ；其次，介绍一些高效的编程模式及其应用，比如使用有限状态机来分析用户数据，使用进程池或线程池来处理用户请求。
正向代理要求客户端自己设置代理服务器的地址。客户的每次请求都将直接发送到该代理服务器，并由代理服务器来请求目标资源。比如处于防火墙内的局域网机器要访问Internet，或者要访问一些被屏蔽掉的国外网站。
反向代理则被设置在服务器端，因而客户端无需进行任何设置。反向代理是指用代理服务器来接收Internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从内部服务器上得到的结果返回给客户端。在这种情况下，代理服务器对外就表现为一个真实的服务器。
Linux服务器程序必须处理三类事件：I/O事件、信号和定时事件。
两种高效的事件处理模式：Reactor和Proactor事件处理模式。同步I/O模型通常用于实现Reactor模式，异步I/O模型则用于实现Proactor模式。
如果程序是计算密集型的，并发编程并没有优势，反而由于任务的切换使效率降低。但如果程序是I/O密集型的，比如经常读写文件、操作数据库等，则情况就不一样了。</description>
    </item>
    
    <item>
      <title>php的压缩函数实现：gzencode、gzdeflate和gzcompress</title>
      <link>http://yezuozuo.github.io/program/php%E7%9A%84%E5%8E%8B%E7%BC%A9%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0gzencodegzdeflate%E5%92%8Cgzcompress/</link>
      <pubDate>Wed, 03 Feb 2016 19:57:44 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E7%9A%84%E5%8E%8B%E7%BC%A9%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0gzencodegzdeflate%E5%92%8Cgzcompress/</guid>
      <description>gzencode 默认使用ZLIB_ENCODING_GZIP编码，使用gzip压缩格式，实际上是使用defalte 算法压缩数据，然后加上文件头和adler32校验 gzdeflate 默认使用ZLIB_ENCODING_RAW编码方式，使用deflate数据压缩算法，实际上是先用 LZ77 压缩，然后用霍夫曼编码压缩 gzcompress ；默认使用ZLIB_ENCODING_DEFLATE编码，使用zlib压缩格式，实际上是用 deflate 压缩数据，然后加上 zlib 头和 CRC 校验  这三个函数的比较实质上是三种压缩方法：deflate, zlib, gzip的比较。
从性能的维度看：deflate 好于 gzip 好于 zlib
从文本文件默认压缩率压缩后体积的维度看：deflate 好于 zlib 好于 gzip
这三种算法中gzip 、zlib的作者都是Jean-Loup Gailly和 Mark Adler。 这两种算法以及图形格式png，使用的压缩算法却都是deflate算法。 deflate算法是同时使用了LZ77算法与哈夫曼编码（Huffman Coding）的一个无损数据压缩算法。 它最初是由Phil Katz为他的PKZIP归档工具第二版所定义的，后来定义在 RFC 1951规范中。
deflate算法的压缩与解压的实现过程可以在压缩库zlib上找到。 PHP的压缩实现依赖于zlib，zlib是一个提供了 deflate, zlib, gzip 压缩方法的函数库。 我们所使用的上面三个函数，将参数中的encoding转为相同，压缩率设置相同，则其最终调用的是同一个函数，效果和性能一样。
PHP的zlib实现是以扩展的方式存在于ext/zlib目录中。通过deflateInit2() + deflate() + deflateEnd()三个函数配合完成压缩功能，inflateInit2() + inflate() + inflateEnd()三个函数配合完成解压功能。压缩最终都是通过php_zlib_encode函数实现调用，除了输入的字符串，压缩率，结果的输出外，不同的入口函数调用参数不同的是其encoding。deflateInit2的第四个参数指定encoding，PHP定义了三个常量：
#define PHP_ZLIB_ENCODING_RAW -0xf //deflate -15 #define PHP_ZLIB_ENCODING_GZIP 0x1f //gzip 15 + 16 #define PHP_ZLIB_ENCODING_DEFLATE 0x0f // zlib 15 三个函数在调用过程可以直接指定encoding使用其它的算法：</description>
    </item>
    
    <item>
      <title>php执行流程</title>
      <link>http://yezuozuo.github.io/program/php%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/</link>
      <pubDate>Mon, 18 Jan 2016 19:54:35 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/</guid>
      <description>一图胜千言，此图详细描述了PHP执行的5个步骤以及过程中做了哪些事情。
以fpm为例：
1、fpm启动时，会先执行 module_startup, 并随着fpm进程常驻
2、当一个请求到达之后，会执行 request_startup, 进行一些请求初始化工作，然后执行代码（execute_script）, 最后，执行request_shutdown，把结果flush, 并做一些收尾工作
3、当我们关闭fpm或reload fpm的时候，会执行module_shutdown
最后抛几个问题给大家思考一下：
1、opcache在哪个阶段，解决了什么问题？
2、ini的文件加载在哪一步？每个请求到达是否都需要解析？
3、当出现fatal error，会有一个register_shutdown_function回调，这个是在哪一步？执行完这个之后，fpm进程还在么？</description>
    </item>
    
    <item>
      <title>爬了人生最大的坑</title>
      <link>http://yezuozuo.github.io/program/%E7%88%AC%E4%BA%86%E4%BA%BA%E7%94%9F%E6%9C%80%E5%A4%A7%E7%9A%84%E5%9D%91/</link>
      <pubDate>Fri, 18 Dec 2015 20:31:47 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E7%88%AC%E4%BA%86%E4%BA%BA%E7%94%9F%E6%9C%80%E5%A4%A7%E7%9A%84%E5%9D%91/</guid>
      <description>刚开始做公司的某个业务的时候，要存数据。
那就存成两部分，一部分zset存列表，一部分string存profile。
zset的key是userid，score是时间，member是profile的key。
很简单吧。
但是，在存string的时候有了麻烦。这次的数据比较多，所以我用了一个redis的集群，按照key%10进行分片。
当然之前都是没问题的，之前一般我们都是拿userid hash的，userid现在最多是10位。但是这次我存的key是trade no。也就是类似这种东西2016121612292709411017316。
所以我就按照上面的方案存了。
之后上线了好久才发现，怎么第7个分片的数据这么多啊，比其他的分片数据多多了。
然后开始查问题。
我们的底层封装的hash有这样一行代码
$res = intval($id); 要把id转换成int。
那么问题来了，2016121612292709411017316转换成int是多少？
 我们存的是字符串，也就是intval(&amp;lsquo;2016121612292709411017316&amp;rsquo;) = 9223372036854775807 这里具体为什么自行Google
 这就是坑</description>
    </item>
    
    <item>
      <title>从一个centos裸机一步一步搭建完整的PHP环境</title>
      <link>http://yezuozuo.github.io/program/%E4%BB%8E%E4%B8%80%E4%B8%AAcentos%E8%A3%B8%E6%9C%BA%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%90%AD%E5%BB%BA%E5%AE%8C%E6%95%B4%E7%9A%84php%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Sat, 28 Nov 2015 13:37:19 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E4%BB%8E%E4%B8%80%E4%B8%AAcentos%E8%A3%B8%E6%9C%BA%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%90%AD%E5%BB%BA%E5%AE%8C%E6%95%B4%E7%9A%84php%E7%8E%AF%E5%A2%83/</guid>
      <description>购买服务器：在阿里云上注册账号，如果是学生的话可以买学生套餐，否则买正常的。在用户面板会得到一个密码，用户名默认为root。这个密码是可以修改的。同时会获得一个公网的IP。 非常重要的就是修改完密码要重启一下服务器。 登陆服务器：在terminal执行ssh root@公网IP( ssh root@42.96.142.34)，提示输入密码，输入之后登陆上。 登录后yum update 配置私钥免登录：如果电脑在已经有了.ssh目录下已经有了.pub文件，那就直接使用即可，没有的话把生成一个公钥和私钥(ssh-keygen)。 在服务器上新建一个用户，比如 adduser zoco 给用户设置密码passwd zoco 输入密码和确认密码 把zoco用户加入wheel用户组 usermod -a -G wheel zoco 用scp把.pub文件传到服务器上（scp id_rsa.pub zoco@42.96.142.34:） 用zoco的身份登陆服务器ssh zoco@42.96.142.34，输入之前的密码 mkdir ~/.ssh touch ~/.ssh/authorized_keys cat ~/id_rsa.pub &amp;raquo; ~/.ssh/authorized_keys 一定要保证在服务器上这个文件所属的用户名是你的用户名（zoco）( chown -R zoco:zoco ~/.ssh )，而且权限设置为700( chown -R 700 ~/.ssh)。(这一步如果权限不够的话用root权限su) 然后在自己.ssh目录下打开config文件，加上 成功的话直接ssh ip( ssh 42.96.142.3)就可以登录到服务器中。 （用root用户）之后就可以禁用root用户登录。(打开/etc/ssh/sshd_config，找到 PermitRootLogin yes 这一句，将yes改成no；)然后禁止密码登录，在相同的文件下找到PasswordAuthentication，改成no。 然后重启ssh一下（ service sshd restart）。 安装nginx:sudo yum install nginx 测试一下nginx的配置文件nginx -t 如果返回这个说明成功 - nginx: the configuration file /etc/nginx/nginx.</description>
    </item>
    
    <item>
      <title>深入php redis pconnect</title>
      <link>http://yezuozuo.github.io/program/%E6%B7%B1%E5%85%A5php-redis-pconnect/</link>
      <pubDate>Thu, 08 Oct 2015 20:31:47 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E6%B7%B1%E5%85%A5php-redis-pconnect/</guid>
      <description>pconnect是phpredis中用于client连接server的api。
API文档中的一句原文：
 The connection will not be closed on close or end of request until the php process ends.
 那么问题来了：
 php process ends是指一次php执行完结，还是fpm的终结？如果是后者，那意味着即使一次php执行完毕，redis连接也不会被释放，下一次执行时redis连接会被重用。 The connection will not be closed on close是 说如果使用了pconnect, 即使在代码中显示的调用close(), 也不会关闭连接？  带着这两个问题，我们做下实验，深入看一下pconnect究竟做了些什么。
准备工作 环境：
nginx + fpm php5.3 我们将fpm配置为
pm.max_children =1 pm.start_servers =1 pm.max_spare_servers =1 这样，我们的页面请求会由一个确定的fpm进程执行，方便strace跟踪。
对应页面请求的php代码：
$ip=“10.136.30.144”; $port=7777; $redis=newRedis(); $redis-&amp;gt;pconnect($ip,$port,1); $key=“test”; $value=“this is test”; $redis-&amp;gt;set($key,$value); $d=$redis-&amp;gt;get($key); var_dump($d); 代码的功能很简单，连接redis，先设置一个值，再取出。
测试问题一 思路：
使用strace观察fpm的系统调用，如果连接的生命周期是一次php执行，那么每次页面调用，都会有connect系统调用，用以连接redis；如果连接的生命周期是fpm的终结，那么只有第一次页面调用会有connect系统调用 ，之后由于连接被重用，无需connect，直接发命令请求即可。 启动一个新的fpm（进程号28082）。</description>
    </item>
    
    <item>
      <title>新浪微博关系服务与Redis的故事</title>
      <link>http://yezuozuo.github.io/program/%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E5%85%B3%E7%B3%BB%E6%9C%8D%E5%8A%A1%E4%B8%8Eredis%E7%9A%84%E6%95%85%E4%BA%8B/</link>
      <pubDate>Wed, 30 Sep 2015 13:34:27 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E5%85%B3%E7%B3%BB%E6%9C%8D%E5%8A%A1%E4%B8%8Eredis%E7%9A%84%E6%95%85%E4%BA%8B/</guid>
      <description>参考：http://os.51cto.com/art/201404/436521.htm
风起 2009年微博刚刚上线的时候，微博关系服务使用的是最传统的 Memcache+Mysql 的方案。Mysql 按 uid hash 进行了分库分表，表结构非常简单：
业务方存在两种查询：
查询用户的关注列表：
select touid from table where fromuid=？order by addTime desc 查询用户的粉丝列表：
select fromuid from table where touid=？order by addTime desc 两种查询的业务需求与分库分表的架构设计存在矛盾，最终导致了冗余存储：以 fromuid 为hash key存一份，以 touid 为hash key再存一份。memcache key 为 fromuid.suffix ，使用不同的 suffix 来区分是关注列表还是粉丝列表，cache value 则为 PHP Serialize 后的 Array。后来为了优化性能，将 value 换成了自己拼装的 byte 数组。
云涌 2011年微博进行平台化改造过程中，业务提出了新的需求：在核心接口中增加了“判断两个用户的关系”的步骤，并增加了“双向关注”的概念。因此两个用户的关系存在四种状态：关注，粉丝，双向关注和无任何关系。为了高效的实现这个需求，平台引入了 Redis 来存储关系。平台使用 Redis 的 hash 来存储关系：key 依然是 uid.suffix，关注列表，粉丝列表及双向关注列表各自有一个不同的 suffix，value 是一个hash，field 是 touid，value 是 addTime。order by addTime 的功能则由 Service 内部 sort 实现。部分大V的粉丝列表可能很长，与产品人员的沟通协商后，将存储限定为“最新的5000个粉丝列表”。</description>
    </item>
    
    <item>
      <title>empty的一个bug</title>
      <link>http://yezuozuo.github.io/program/empty%E7%9A%84%E4%B8%80%E4%B8%AAbug/</link>
      <pubDate>Mon, 31 Aug 2015 20:13:02 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/empty%E7%9A%84%E4%B8%80%E4%B8%AAbug/</guid>
      <description>先看一段代码例子：
class x { public function __get($k) { return 1; } public function __isset($k) { return true; } } $x = new x(); var_dump($x-&amp;gt;b); var_dump(empty($x-&amp;gt;b)); 这个返回 1 false
把__isset的代码注释掉，例子为：
class x { public function __get($k) { return 1; } //public function __isset($k) { // return true; //} } $x = new x(); var_dump($x-&amp;gt;b); var_dump(empty($x-&amp;gt;b)); 返回变为 1 true
原因为：
empty() will call __isset() first, and only if it returns true will it call __get().</description>
    </item>
    
    <item>
      <title>一个Bad gateway的bug</title>
      <link>http://yezuozuo.github.io/program/%E4%B8%80%E4%B8%AAbad-gateway%E7%9A%84bug/</link>
      <pubDate>Thu, 16 Jul 2015 19:48:25 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/%E4%B8%80%E4%B8%AAbad-gateway%E7%9A%84bug/</guid>
      <description>线上机器有回调的接口，会时不时的报502 Bad Gateway的错误，本来也没什么，但是天天每隔几分钟来一条报警的短信，很烦的。让sa那边把报警取消了他们又说这个业务不能取消，只能尝试解决一下了。
首先上网查了一下，都是什么配置错误啊，程序错误啊之类的。
但是我们的问题诡异就诡异在：PHP层面没有任何报错的信息。而且配置错误是一个非常小概率的事情，差不多100台机器只有两台报警而且不会影响到线上的业务。（最主要的原因是改配置的话需要所有机器都改一遍，在流量小的情况下是不会报警的，测试风险和成本太大，所以只能从别的地方入手）。
遇到bug不可怕，但是那种没有线索的bug才是让人头疼的。
那么既然php层面没有问题，那么就来看nginx层面，通过对报警时间和日志的分析，发现报警前几乎error.log里面都会有这样一段日志：
2015/07/15 09:24:04 [error] 10853#0: *6453 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 219.136.34.244, server: 203.20.249.124, request: &amp;quot;POST /api.php/pay/uc/callback HTTP/1.1&amp;quot;, upstream: &amp;quot;fastcgi://127.0.0.1:7777&amp;quot;, host: &amp;quot;203.20.249.124:81&amp;quot; 这是nginx的错误日志, 看字面意思是nginx发现没有存活的后端了，但是很奇怪的事情是，这段时间一直访问都正常。
现在只能从nginx源码的角度来看了。
因为是upstream有关的报错，所以在ngx_http_upstream.c中查找“no live upstreams”的关键字，可以找到如下代码（其实，你会发现，如果在nginx全局代码中找的话，也只有这个文件里面有这个关键字）：
rc = ngx_event_connect_peer(&amp;amp;u-&amp;gt;peer); ngx_log_debug1(NGX_LOG_DEBUG_HTTP, r-&amp;gt;connection-&amp;gt;log, 0, &amp;quot;http upstream connect: %i&amp;quot;, rc); if (rc == NGX_ERROR) { ngx_http_upstream_finalize_request(r, u, NGX_HTTP_INTERNAL_SERVER_ERROR); return; } u-&amp;gt;state-&amp;gt;peer = u-&amp;gt;peer.name; if (rc == NGX_BUSY) { ngx_log_error(NGX_LOG_ERR, r-&amp;gt;connection-&amp;gt;log, 0, &amp;quot;no live upstreams&amp;quot;); ngx_http_upstream_next(r, u, NGX_HTTP_UPSTREAM_FT_NOLIVE); return; } if (rc == NGX_DECLINED) { ngx_http_upstream_next(r, u, NGX_HTTP_UPSTREAM_FT_ERROR); return; } 在这里可以看出，当rc等于NGX_BUSY的时候，就会记录“no live upstreams”的错误。</description>
    </item>
    
    <item>
      <title>xhprof源码分析</title>
      <link>http://yezuozuo.github.io/program/xhprof%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Thu, 04 Jun 2015 19:48:25 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/xhprof%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>源码位置：/path/xhprof/extension/xhprof.c
数据结构 两个重要的数据结构：
typedef struct hp_entry_t { char *name_hprof; /* function name */ int rlvl_hprof; /* recursion level for function */ uint64 tsc_start; /* start value for TSC counter */ long int mu_start_hprof; /* memory usage */ long int pmu_start_hprof; /* peak memory usage */ struct rusage ru_start_hprof; /* user/sys time start */ struct hp_entry_t *prev_hprof; /* ptr to prev entry being profiled */ uint8 hash_code; /* hash_code for the function name */ } hp_entry_t; typedef struct hp_global_t { /* ---------- Global attributes: ----------- */ /* Indicates if xhprof is currently enabled */ int enabled; /* Indicates if xhprof was ever enabled during this request */ int ever_enabled; /* Holds all the xhprof statistics */ zval *stats_count; /* Indicates the current xhprof mode or level */ int profiler_level; /* Top of the profile stack */ hp_entry_t *entries; /* freelist of hp_entry_t chunks for reuse.</description>
    </item>
    
    <item>
      <title>Laravel学习笔记——神奇的服务容器</title>
      <link>http://yezuozuo.github.io/program/laravel%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%A5%9E%E5%A5%87%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%AE%B9%E5%99%A8/</link>
      <pubDate>Tue, 05 May 2015 09:50:26 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/laravel%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E7%A5%9E%E5%A5%87%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%AE%B9%E5%99%A8/</guid>
      <description>参考：https://www.insp.top/learn-laravel-container
 容器，字面上理解就是装东西的东西。常见的变量、对象属性等都可以算是容器。一个容器能够装什么，全部取决于你对该容器的定义。当然，有这样一种容器，它存放的不是文本、数值，而是对象、对象的描述（类、接口）或者是提供对象的回调，通过这种容器，我们得以实现许多高级的功能，其中最常提到的，就是 “解耦” 、“依赖注入（DI）”。本文就从这里开始。
 IoC 容器， laravel 的核心 Laravel 的核心就是一个 IoC 容器，根据文档，称其为“服务容器”，顾名思义，该容器提供了整个框架中需要的一系列服务。作为初学者，很多人会在这一个概念上犯难，因此，我打算从一些基础的内容开始讲解，通过理解面向对象开发中依赖的产生和解决方法，来逐渐揭开“依赖注入”的面纱，逐渐理解这一神奇的设计理念。
IoC 容器诞生的故事 面向对象编程，有以下几样东西无时不刻的接触：接口、类还有对象。这其中，接口是类的原型，一个类必须要遵守其实现的接口；对象则是一个类实例化后的产物，我们称其为一个实例。
我们把一个“超人”作为一个类，
class Superman {} 我们可以想象，一个超人诞生的时候肯定拥有至少一个超能力，这个超能力也可以抽象为一个对象，为这个对象定义一个描述他的类吧。一个超能力肯定有多种属性、（操作）方法，这个尽情的想象，但是目前我们先大致定义一个只有属性的“超能力”，至于能干啥，我们以后再丰富：
class Power { /** * 能力值 */ protected $ability; /** * 能力范围或距离 */ protected $range; public function __construct($ability, $range) { $this-&amp;gt;ability = $ability; $this-&amp;gt;range = $range; } } 这时候我们回过头，修改一下之前的“超人”类，让一个“超人”创建的时候被赋予一个超能力：
class Superman { protected $power; public function __construct() { $this-&amp;gt;power = new Power(999, 100); } } 这样的话，当我们创建一个“超人”实例的时候，同时也创建了一个“超能力”的实例，但是，我们看到了一点，“超人”和“超能力”之间不可避免的产生了一个依赖。
 所谓“依赖”，就是 “我若依赖你，我就不能离开你”。</description>
    </item>
    
    <item>
      <title>php黑魔法</title>
      <link>http://yezuozuo.github.io/program/php%E9%BB%91%E9%AD%94%E6%B3%95/</link>
      <pubDate>Mon, 13 Apr 2015 14:00:02 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E9%BB%91%E9%AD%94%E6%B3%95/</guid>
      <description>猜猜 echo 1&amp;hellip;1输出多少？
10.1
1.意思是1.0，.1意思是0.1，中间的点是连接字符串、前浮点数转成1而后浮点数转换成字符串，结果成了&#39;1&amp;rsquo;.&amp;lsquo;0.1&amp;rsquo;，最终变成了10.1的字符串</description>
    </item>
    
    <item>
      <title>PHP是世界上最好的语言！之array_merge</title>
      <link>http://yezuozuo.github.io/program/php%E6%98%AF%E4%B8%96%E7%95%8C%E4%B8%8A%E6%9C%80%E5%A5%BD%E7%9A%84%E8%AF%AD%E8%A8%80%E4%B9%8Barray_merge/</link>
      <pubDate>Mon, 02 Mar 2015 13:53:06 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E6%98%AF%E4%B8%96%E7%95%8C%E4%B8%8A%E6%9C%80%E5%A5%BD%E7%9A%84%E8%AF%AD%E8%A8%80%E4%B9%8Barray_merge/</guid>
      <description>最近在使用函数 array_merge 合并两个数组时，得到的结果总是不确定，代码如这样：
$arrResult = array_merge($arrInput1, $arrInput2); 如果已踩过这个坑，肯定能一眼看出代码的潜在问题，以及如何避免，但是问题究竟是什么呢？变量 $arrResult 的值会根据函数 array_merge 的两个参数的类型及值而变化，如：键为数字的字典、null 等，详细问题解释说明请继续往下看！
来自 PHP 官方网站的array_merge函数说明
 函数功能：合并一个或多个数组
 函数原型：
array array_merge(array $array1[, array $...]) 函数 array_merge 将一个或多个数组的单元合并，后一个数组中的值追加到前一个数组的后面，并作为函数结果返回。
示例场景1：如果输入数组中有相同的字符串键名，则该键名后面的值将覆盖前一个值，如： &amp;lt;?php // PHP 5.6.15 $a = array(&#39;hello&#39; =&amp;gt; 1, &#39;world&#39; =&amp;gt; 2); $b = array(&#39;hi&#39; =&amp;gt; 1, &#39;world&#39; =&amp;gt; 3); $c = array_merge($a, $b); var_dump($c); $d = array_merge($b, $a); var_dump($d); 执行脚本后的结果为：
array(3) { [&amp;quot;hello&amp;quot;]=&amp;gt; int(1) [&amp;quot;world&amp;quot;]=&amp;gt; int(3) [&amp;quot;hi&amp;quot;]=&amp;gt; int(1) } array(3) { [&amp;quot;hi&amp;quot;]=&amp;gt; int(1) [&amp;quot;world&amp;quot;]=&amp;gt; int(2) [&amp;quot;hello&amp;quot;]=&amp;gt; int(1) } 示例场景2：如果输入数组只有一个且为数字索引，即其中一个数组为 array()，则键名会以连续方式重新索引，如： &amp;lt;?</description>
    </item>
    
    <item>
      <title>PHP是世界上最好的语言！之==</title>
      <link>http://yezuozuo.github.io/program/php%E6%98%AF%E4%B8%96%E7%95%8C%E4%B8%8A%E6%9C%80%E5%A5%BD%E7%9A%84%E8%AF%AD%E8%A8%80%E4%B9%8B/</link>
      <pubDate>Mon, 02 Mar 2015 13:52:31 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E6%98%AF%E4%B8%96%E7%95%8C%E4%B8%8A%E6%9C%80%E5%A5%BD%E7%9A%84%E8%AF%AD%E8%A8%80%E4%B9%8B/</guid>
      <description>我决定从“==”说起
PHP 是弱类型的语言，会自动进行数据类型转换，这无疑给我们的开发带来了极大的方便。可事实真是如此吗？那就先就从==说起。
首先，看一下这段代码。猜猜看结果会是什么
&amp;lt;?php var_dump(md5(&#39;240610708&#39;) == md5(&#39;QNKCDZO&#39;)); var_dump(md5(&#39;aabg7XSs&#39;) == md5(&#39;aabC9RqS&#39;)); var_dump(sha1(&#39;aaroZmOk&#39;) == sha1(&#39;aaK1STfY&#39;)); var_dump(sha1(&#39;aaO8zKZF&#39;) == sha1(&#39;aa3OFF9m&#39;)); var_dump(&#39;0010e2&#39; == &#39;1e3&#39;); var_dump(&#39;0x1234Ab&#39; == &#39;1193131&#39;); var_dump(&#39;0xABCdef&#39; == &#39; 0xABCdef&#39;); var_dump(0 == &#39;abcdefg&#39;); var_dump(1 == &#39;1abcdef&#39;); ?&amp;gt; 一眼看过，很明显肯定都是false吧，但运行代码后发现全是true！
WTF!
为什么会这样？ PHP 是弱类型的语言。使用==对比两个变量时，当有一个变量为整数，另外一个变量也会转换为整数。这也就解释了，为什么0 == &amp;lsquo;abcdefg&amp;rsquo;和1 == &amp;lsquo;1abcdef&amp;rsquo;会成立。
但是，其他的代码呢？字符串难道还会转换？
PHP 手册上为我们提供了解释说明。
 If you compare a number with a string or the comparison involves numerical strings, then each string is converted to a number and the comparison performed numerically.</description>
    </item>
    
    <item>
      <title>PHP是世界上最好的语言！之……</title>
      <link>http://yezuozuo.github.io/program/php%E6%98%AF%E4%B8%96%E7%95%8C%E4%B8%8A%E6%9C%80%E5%A5%BD%E7%9A%84%E8%AF%AD%E8%A8%80%E4%B9%8B/</link>
      <pubDate>Mon, 02 Mar 2015 13:50:33 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E6%98%AF%E4%B8%96%E7%95%8C%E4%B8%8A%E6%9C%80%E5%A5%BD%E7%9A%84%E8%AF%AD%E8%A8%80%E4%B9%8B/</guid>
      <description>1.++与&amp;ndash; &amp;lt;?php $a = null; $a ++; var_dump($a); echo &amp;quot;\n&amp;quot;; $a = null; $a --; var_dump($a); 执行结果为
int(1) NULL ++的时候为1我还可以理解 &amp;ndash;的时候为null我就凌乱了
解决这种++和&amp;ndash;中的不一致的办法就是根本不用它们，用+=和-=代替。
2.strrchr函数 官方解释
 strrchr() 函数查找字符串在另一个字符串中最后一次出现的位置，并返回从该位置到字符串结尾的所有字符。 如果成失败，否则返回 false。
 实际上，这个函数是查找某个字符，而不是查找字符串
&amp;lt;?php $a = &#39;abcdef.txt&#39;; $b = &#39;.php&#39;; echo strrchr($a, $b); 上面的代码输出是：.txt 也就是说，如果$b是字符串，只使用第一个字符，后面的其它字符会忽略
3.trim函数遇到中文空格时，会乱码 &amp;lt;?php $str = &#39; 《前后有全半角空格》　&#39;; var_dump($str); $str2 = trim($str, &#39; &#39;); var_dump($str2); 执行结果为：
string(38) &amp;quot; 《前后有全半角空格》　&amp;quot; string(28) &amp;quot;�前后有全半角空格》&amp;quot; 这个问题的修改方法是采用正则
&amp;lt;?php $str = &#39; 《前后有全半角空格》　&#39;; var_dump($str); $str3 = mb_ereg_replace(&#39;^(?</description>
    </item>
    
    <item>
      <title>php语法优化实践</title>
      <link>http://yezuozuo.github.io/program/php%E8%AF%AD%E6%B3%95%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Thu, 19 Feb 2015 13:44:14 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E8%AF%AD%E6%B3%95%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/</guid>
      <description>1. 避免使用array_walk_recursive或其他形式的递归 比如想要对一个多位数组的内容进行过滤 使用array_walk_recursive的写法为
function outputFilter(&amp;amp;$value) { $value = preg_replace(&#39;&#39;, &#39;&#39;, $value); } } array_walk_recursive($output, &#39;outputFilter&#39;); 其实我们可以把多维数组降成一维，使用array_reduce或者json_encode将数组变为字符串进行处理。
function outputFilter_u(&amp;amp;$value) { $value = preg_replace(&#39;&#39;, &#39;&#39;, $value); } $json = json_encode($output); outputFilter_u($json); 2. 用isset代替in_array 尤其是越大的数组这种对比越明显
避免
if (in_array(&#39;1&#39;, $array)) {} 使用
if (isset($array[&#39;1&#39;])) {} 3. 少用@错误抑制符 虽然会减少了warning和notice，但是实际上会带来性能的下降
避免
@test(); 4.避免Deep Array 对于C来说, 符号在执行器都会变成地址(绝大部分), 而对于PHP来说, 符号都需要经过查找(Hash Lookup)才能使用, 于是我也看到了类似下面的代码
for ($i=0; i&amp;lt;10;i++) { $arr[1][2][3][4][5] = $i; } 这样的话，每一次循环, 都会带来6次的Hash Lookup…..
5. 变量先定义再引用 这样不仅会在性能上得到一定的提升，而且最重要的是代码的可读性会提升，毕竟看代码看的看的突然出现一个不知道什么意思的变量，你懂的……
6. 用isset($string[5])代替strlen($string)（不建议在生产环境使用） 避免</description>
    </item>
    
    <item>
      <title>php与rand相关的函数</title>
      <link>http://yezuozuo.github.io/program/php%E4%B8%8Erand%E7%9B%B8%E5%85%B3%E7%9A%84%E5%87%BD%E6%95%B0/</link>
      <pubDate>Mon, 12 Jan 2015 14:00:30 +0800</pubDate>
      
      <guid>http://yezuozuo.github.io/program/php%E4%B8%8Erand%E7%9B%B8%E5%85%B3%E7%9A%84%E5%87%BD%E6%95%B0/</guid>
      <description>mcrypt_create_iv make_http_soap_request shuffle array_rand php_password_make_salt rand crypt 这些函数在php底层都用到了php_rand的方法，会导致随机数随机不均匀的问题，慎用。</description>
    </item>
    
  </channel>
</rss>